var documenterSearchIndex = {"docs":
[{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"EditURL=\"datatoml.org\"","category":"page"},{"location":"datatoml/#Data.toml","page":"Data.toml format","title":"Data.toml","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Data collections are represented on-disk as Data.toml files. While DataToolkit can be used at a basic level without any knowledge of the structure of the file, a little knowledge goes a long way (for instance when editing a dataset).","category":"page"},{"location":"datatoml/#Overall-structure","page":"Data.toml format","title":"Overall structure","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"See the TOML refresher below if you're a bit rusty, then come back to this.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"A Data.toml file is broadly composed of three sections:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Global setup information\nConfiguration\nDatasets","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Here's what that structure looks like in practice:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"data_config_version=0\n\nname=\"data collection name\"\nuuid=\"a UUIDv4\"\nplugins=[\"plugin1\", \"plugin2\", ...]\n\n[config]\n# [Properties of the data collection itself]\n\n[[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...\n\n[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype=[\"a QualifiedType\", ...]\npriority=1 # (optional)\n# other properties...\n\n[[mydataset]]\n# There may be multiple data sets by the same name,\n# but they must be uniquely identifyable by their properties\n\n[[exampledata]]\n# Another data set","category":"page"},{"location":"datatoml/#Global-setup","page":"Data.toml format","title":"Global setup","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"There are four top-level non-table properties currently recognised:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"The Data.toml format version (data_config_version)\nThe name (name) and UUID (uuid) of the data collection:\nname is a string that provides a nice human-readable label for the data collection. It may contain any character but :, however sticking to [A-Za-z0-9_] is recommended.\nuuid is a UUIDv4 that canonically identifies the data collection\nThe plugins used by the data collection (plugins): a list of strings.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"This should all be set as a matter of course.","category":"page"},{"location":"datatoml/#Configuration","page":"Data.toml format","title":"Configuration","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"The config TOML table is special, and is used to hold custom attributes of the data collection, for example:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[config]\nmykey=\"value\"\n\n[config.defaults]\ndescription=\"Ooops, somebody forgot to describe this.\"\n\n[config.defaults.storage.filesystem]\npriority=2","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"As a consequence of this, no dataset may be named \"config\".","category":"page"},{"location":"datatoml/#Reserved-configuration-attributes","page":"Data.toml format","title":"Reserved configuration attributes","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"The config.locked attribute is used by DataToolkitCore to indicate that the Data.toml file should not be modified, and to override it the attribute must be changed within the Data.toml file. By setting config.locked = true, you protect yourself from accidental modifications to the data file.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"notes: Notes\nThis functionality is part of DataToolkitCore because it is taken into account in the implementation of Base.iswritable(::DataCollection).","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Other attributes may be used by plugins to affect their behaviour. For instance, the defaults plugin uses the config.defaults attribute, and both the store and cache plugins use the config.store attribute.","category":"page"},{"location":"datatoml/#Datasets","page":"Data.toml format","title":"Datasets","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"All datasets are represented using an array of tables. This allows multiple datasets to have the same name, and be distinguished by other attributes (e.g. version information). The dataset name can be any string that does not contain a colon (:), other than config.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Besides uuid, the attributes storage, loader, and writer are reserved for specifying transformers. All datasets must have a uuid key, this is important for providing a canonical unique reference to a particular dataset. All other attributes are free to be used by plugins or to add arbitrary metadata.","category":"page"},{"location":"datatoml/#Data-transformers","page":"Data.toml format","title":"Data transformers","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"The transformers of a dataset (storage, loader, and writer) are specified using sub-tables of the dataset. Any number of transformers can be specified, and the first applicable (sorted by priority, lowest winning) will be used.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype = [\"list\", \"of\", \"types\"] # (optional)\npriority=1 # (optional)\n# other properties...","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"The type attribute (either a single type, or a list) is given a default value inferred from loaded code relating to the transformer. In situations where this is unsatisfactory, it can be explicitly specified.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"All transformers must set the driver key. All attributes other than driver, type, and priority are free to be used by the transformer and plugins.","category":"page"},{"location":"datatoml/#TOML-refresher","page":"Data.toml format","title":"TOML refresher","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"TOML files are already widely used with Julia (for example, Project.toml and Manifest.toml) files, as they strike a good compromise between capability and complexity. See the TOML documentation for a full description of the format, but here are the components most relevant to Data.toml files.","category":"page"},{"location":"datatoml/#Key-value-pairs","page":"Data.toml format","title":"Key-value pairs","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"key = \"value\"","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"This represents a \"key\" dictionary key having the value \"value\". Strings, numbers, booleans, and date/time stamps are all appropriate value forms.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"a = \"value\"\nb = 2\nc = 3.1e+12\nd = true\ne = 1979-05-27T07:32:00Z","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Arrays are written using [ ] syntax, and can spread across multiple lines.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"key = [1, 2, 3]","category":"page"},{"location":"datatoml/#Tables-(Dictionaries)","page":"Data.toml format","title":"Tables (Dictionaries)","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"A collection of key-value pairs within a certain scope form a Julia Dict when parsed. TOML allows for nested dictionaries using tables. A new table is created with a bracketed header line, like so:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[new_table]","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"All key-value entries after such a table header, up to the next table header, belong to that table. For example:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[mytable]\na = 1\nb = 2","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"this is parsed as","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Dict(\"mytable\" => Dict(\"a\" => 1, \"b\" => 2))","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"It is also possible to represent this using dotted keys, e.g.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"mytable.a = 1\nmytable.b = 2","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"These two styles can mixed to form nested tables.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[mytable.innertable.deeply_nested]\nkey = \"value\"","category":"page"},{"location":"datatoml/#Arrays-of-tables","page":"Data.toml format","title":"Arrays of tables","text":"","category":"section"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"A list of dictionaries (array of tables in TOML terminology) can be formed using double-bracketed headers, e.g.","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[[table_array]]","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"All double-bracketed tables will be collected together into an array, for example:","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"[[table_array]]\nkey = 1\n\n[[table_array]]\nkey = 2","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"will be parsed as","category":"page"},{"location":"datatoml/","page":"Data.toml format","title":"Data.toml format","text":"Dict(\"table_array\" => [Dict(\"key\" => 1),\n                       Dict(\"key\" => 2)])","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"EditURL=\"quickref.org\"","category":"page"},{"location":"quickref/#Quick-Reference-Guide","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"This gives the bare essentials, as the relevant Data REPL command (enter the Data REPL with }) and Julia function when sensible.","category":"page"},{"location":"quickref/#Data-REPL-help","page":"Quick Reference Guide","title":"Data REPL help","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Look at the REPL help docs, accessible within the Data REPL.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"(demo) data> help help","category":"page"},{"location":"quickref/#Accessing-a-dataset","page":"Quick Reference Guide","title":"Accessing a dataset","text":"","category":"section"},{"location":"quickref/#In-the-default-form","page":"Quick Reference Guide","title":"In the default form","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Using the Data REPL.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"(demo) data> show <identifier>","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Within a program.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"d\"<identifier>\"\nread(dataset(\"<identifier>\"))","category":"page"},{"location":"quickref/#As-a-particular-type","page":"Quick Reference Guide","title":"As a particular type","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Either add ::<type> to the identifier string, or if using read provide the type as the second argument, i.e.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"read(dataset(\"<identifier>\"), TYPE)","category":"page"},{"location":"quickref/#Creating-a-new-dataset","page":"Quick Reference Guide","title":"Creating a new dataset","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Using the Data REPL.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"(demo) data> add <name> <source>","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Within a program.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"DataToolkit.Base.add(DataSet, \"<name>\", Dict{String, Any}(), \"<source>\"; ...)","category":"page"},{"location":"quickref/#Loading-a-data-collection","page":"Quick Reference Guide","title":"Loading a data collection","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Using the Data REPL","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"(⋅) data> stack load <path>","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Within a program.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"loadcollection!(\"<path>\")","category":"page"},{"location":"quickref/#Creating-a-data-collection","page":"Quick Reference Guide","title":"Creating a data collection","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Using the Data REPL.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"(⋅) data> init <name>","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Within a program.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"DataToolkit.create!(DataCollection, \"<name>\", \"<path>\")","category":"page"},{"location":"quickref/#Using-a-package-within-a-julia-loader-script","page":"Quick Reference Guide","title":"Using a package within a julia loader script","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Use @require SomePkg instead of import SomePkg (and don't use using).","category":"page"},{"location":"quickref/#Registering-a-package-for-use-with-@require","page":"Quick Reference Guide","title":"Registering a package for use with @require","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"Call DataToolkit.@addpkgs A B C..., or to make all direct dependencies of the current module available: DataToolkit.@addpkgs *.","category":"page"},{"location":"quickref/#Using-the-Data-REPL-within-code","page":"Quick Reference Guide","title":"Using the Data REPL within code","text":"","category":"section"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"The cmd macro data...`` allows for Data REPL commands to be easily inserted within a program.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"This also makes it relatively simple to invoke Data REPL functions from the shell.","category":"page"},{"location":"quickref/","page":"Quick Reference Guide","title":"Quick Reference Guide","text":"~$ julia -e 'using DataToolkit; data`stuff...`'","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"EditURL=\"reference.org\"","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"This is the public API for DataToolkit. Some symbols have been exported for convenience, others need to be specifically imported or accessed with DataToolkit.<thing>.","category":"page"},{"location":"reference/#Exported-Symbols","page":"Reference","title":"Exported Symbols","text":"","category":"section"},{"location":"reference/#Macros","page":"Reference","title":"Macros","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"@d_str\n@data_cmd\n@require","category":"page"},{"location":"reference/#DataToolkit.@d_str","page":"Reference","title":"DataToolkit.@d_str","text":"@d_str -> loaded data\n\nShorthand for loading a dataset in the default format, d\"iris\" is equivalent to read(dataset(\"iris\")).\n\n\n\n\n\n","category":"macro"},{"location":"reference/#DataToolkit.@data_cmd","page":"Reference","title":"DataToolkit.@data_cmd","text":"@data_cmd -> Data REPL command result\n\nProxy for running the command in the Data REPL, e.g. data`config set demo 1` is equivalent to data> config set demo 1.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#DataToolkit.@require","page":"Reference","title":"DataToolkit.@require","text":"@require Package\n@require Package = \"UUID\"\n\nRequire the package Package, either previously registered with @addpkg or by UUID.\n\nThis sets a variable Package to the module of the package.\n\nIf the package is not currently loaded, DataToolkit will attempt to lazy-load the package via an early return PkgRequiredRerunNeeded singleton. So long as this is seen by a calling invokepkglatest the package will be loaded and the function re-run.\n\nSee also: @addpkg, invokepkglatest.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#Functions","page":"Reference","title":"Functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"dataset\nloadcollection!","category":"page"},{"location":"reference/#DataToolkit.dataset","page":"Reference","title":"DataToolkit.dataset","text":"dataset([collection::DataCollection], identstr::AbstractString, [parameters::Dict{String, Any}])\ndataset([collection::DataCollection], identstr::AbstractString, [parameters::Pair{String, Any}...])\n\nReturn the data set identified by identstr, optionally specifying the collection the data set should be found in and any parameters that apply.\n\n\n\n\n\n","category":"function"},{"location":"reference/#DataToolkitCore.loadcollection!","page":"Reference","title":"DataToolkitCore.loadcollection!","text":"loadcollection!(source::Union{<:AbstractString, <:IO}, mod::Module=Base.Main;\n                soft::Bool=false, index::Int=1)\n\nLoad a data collection from source and add it to the data stack at index. source must be accepted by read(source, DataCollection).\n\nmod should be set to the Module within which loadcollection! is being invoked. This is important when code is run by the collection. As such, it is usually appropriate to call:\n\nloadcollection!(source, @__MODULE__; soft)\n\nWhen soft is set, should an data collection already exist with the same UUID, nothing will be done and nothing will be returned.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Types","page":"Reference","title":"Types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"# DataToolkitCore.DataSet","category":"page"},{"location":"reference/#Unexported-Symbols","page":"Reference","title":"Unexported Symbols","text":"","category":"section"},{"location":"reference/#Modules","page":"Reference","title":"Modules","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"DataToolkitBase and DataToolkitCommon are available as Base and Common respectively.","category":"page"},{"location":"reference/#Macros-2","page":"Reference","title":"Macros","text":"","category":"section"},{"location":"reference/#DataToolkit","page":"Reference","title":"DataToolkit","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"DataToolkit.@addpkgs\nDataToolkit.@addpkg","category":"page"},{"location":"reference/#DataToolkit.@addpkgs","page":"Reference","title":"DataToolkit.@addpkgs","text":"@addpkgs pkgs...\n\nFor each named package, register it with DataToolkitBase. Each package must be a dependency of the current module, recorded in its Project.toml.\n\nThis allows the packages to be used with DataToolkitBase.@require.\n\nInstead of providing a list of packages, the symbol * can be provided to register all dependencies.\n\nThis must be run at runtime to take effect, so be sure to place it in the __init__ function of a package.\n\nExamples\n\n@addpkgs JSON3 CSV\n@addpkgs * # Register all dependencies\n\n\n\n\n\n","category":"macro"},{"location":"reference/#DataToolkit.@addpkg","page":"Reference","title":"DataToolkit.@addpkg","text":"@addpkg name::Symbol uuid::String\n\nRegister the package identified by name with UUID uuid. This package may now be used with @require $name.\n\nAll @addpkg statements should lie within a module's __init__ function.\n\nExample\n\n@addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n\nSee also: @require, addpkg.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#Functions-2","page":"Reference","title":"Functions","text":"","category":"section"},{"location":"reference/#DataToolkit-2","page":"Reference","title":"DataToolkit","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"DataToolkit.create!(::Type{DataCollection}, ::Union{String, Nothing}, ::Union{String, Nothing})\nDataToolkit.plugins\nDataToolkit.addpkgs","category":"page"},{"location":"reference/#DataToolkitCore.create!-Tuple{Type{DataCollection}, Union{Nothing, String}, Union{Nothing, String}}","page":"Reference","title":"DataToolkitCore.create!","text":"create!(::Type{DataCollection}, name::Union{String, Nothing}, path::Union{String, Nothing};\n        uuid::UUID=uuid4(), plugins::Vector{String}=String[], mod::Module=Base.Main)\n\nCreate a new data collection.\n\nThis can be an in-memory data collection, when path is set to nothing, or a collection which corresponds to a Data TOML file, in which case path should be set to either a path to a .toml file or an existing directory in which a Data.toml file should be placed.\n\nWhen a path is provided, the data collection will immediately be written, overwriting any existing file at the path.\n\n\n\n\n\n","category":"method"},{"location":"reference/#DataToolkit.plugins","page":"Reference","title":"DataToolkit.plugins","text":"plugins()\n\nList the currently availible plugins, by name.\n\n\n\n\n\n","category":"function"},{"location":"reference/#DataToolkit.addpkgs","page":"Reference","title":"DataToolkit.addpkgs","text":"addpkgs(mod::Module, pkgs::Vector{Symbol})\n\nFor each package in pkgs, which are dependencies recorded in mod's Project.toml, register the package with DataToolkitBase.addpkg.\n\nIf pkgs consists of the single symbol :*, then all dependencies of mod will be registered.\n\nThis must be run at runtime to take effect, so be sure to place it in the __init__ function of a package.\n\n\n\n\n\n","category":"function"},{"location":"reference/#DataToolkitBase","page":"Reference","title":"DataToolkitBase","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"DataToolkitCore.getlayer","category":"page"},{"location":"reference/#DataToolkitCore.getlayer","page":"Reference","title":"DataToolkitCore.getlayer","text":"getlayer([stack])\n\nReturn the first DataCollection on the stack.\n\nstack defaults to STACK, and must be a Vector{DataCollection}.\n\n\n\n\n\ngetlayer([stack], name::AbstractString)\ngetlayer([stack], uuid::UUID)\n\nFind the DataCollection in STACK with name/uuid.\n\nstack defaults to STACK, and must be a Vector{DataCollection}.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Types-2","page":"Reference","title":"Types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"# DataToolkitCore.DataCollection\n# DataToolkitCore.DataSet\n# DataToolkitCore.DataStorage\n# DataToolkitCore.DataLoader\n# DataToolkitCore.DataWriter\nDataToolkitCore.Identifier","category":"page"},{"location":"reference/#DataToolkitCore.Identifier","page":"Reference","title":"DataToolkitCore.Identifier","text":"Identifier\n\nA description that can be used to uniquely identify a DataSet.\n\nFour fields are used to describe the target DataSet:\n\ncollection, the name or UUID of the collection (optional).\ndataset, the name or UUID of the dataset.\ntype, the type that should be loaded from the dataset.\nparameters, any extra parameters of the dataset that should match.\n\nSee also: resolve, refine.\n\nConstructors\n\nIdentifier(collection::Union{String, UUID, Nothing},\n           dataset::Union{String, UUID},\n           type::Union{QualifiedType, Nothing},\n           parameters::Dict{String, Any})\n\nParsing\n\nAn Identifier can be represented as a string with the following form, with the optional components enclosed by square brackets:\n\n[COLLECTION:]DATASET[::TYPE]\n\nSuch forms can be parsed to an Identifier by simply calling the parse function, i.e. parse(Identifier, \"mycollection:dataset\").\n\n\n\n\n\nIdentifier(dataset::DataSet, collection::Union{Symbol, Nothing}=:name,\n           name::Symbol=something(collection, :name))\n\nCreate an Identifier referring to dataset, specifying the collection dataset comes from as well (when collection is not nothing) as all of its parameters, but without any type information.\n\nShould collection and name default to the symbol :name, which signals that the collection and dataset reference of the generated Identifier should use the names of the collection/dataset. If set to :uuid, the UUID is used instead. No other value symbols are supported.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Introduction","title":"Introduction","text":"EditURL=\"index.org\"","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"DataToolkit is a batteries-included family of packages for robustly managing data. The particular package(s) you want to use will depend on the project.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Use DataToolkit for analysis projects and scripts\nUse DataToolkitBase when making a package that needs data\nOptionally, use DataToolkitDocumenter too to document the datasets\nUse DataToolkitCore when making a package extending DataToolkit, and possibly DataToolkitStore too.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Whether using DataToolkit or DataToolkitBase, to see what storage providers and formats are supported out-of-the-box, look at the DataToolkitCommon documentation.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"#Why-this-exists","page":"Introduction","title":"Why this exists","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Poor data management approaches are rampant. This is well-recognised, and so people have created tools that attempt to tackle subsets of the problem — such as DataLad, DVC, the Kedro data catalogue, Snakemake, Nextflow, Intake, Pkg.jl's Artifacts, and DataSets.jl. These tools contain many good ideas, but all fall short of the combination of convenience and robustness that is possible.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"DataToolkit leverages key Julia features —reproducible package management with Pkg.jl, independence from system state with JLL packages, and well-managed environments— to push the envelope on how easily data can be robustly managed. The three tenets of the project are reproducibility, flexibility, and convenience.","category":"page"},{"location":"#Declarative-data-management","page":"Introduction","title":"Declarative data management","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"DataToolkit takes a declarative approach to data management, and represents collections of datasets in TOML files. To give a taste of what this system looks like in practice, here's a sample TOML representation of a dataset (iris).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    checksum = \"crc32c:d5c06b86\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"\n    args.header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]\n    args.skipto = 2","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This creates an iris dataset that can be conveniently and reproducibly loaded within a Julia session, with data flowing like so:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"#The-data-model","page":"Introduction","title":"The data model","text":"","category":"section"},{"location":"#Data-Sets-—-Information-and-Data","page":"Introduction","title":"Data Sets — Information and Data","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Useful information is a particular representation of basic data. We acquire data and load it into a more informative form, and similarly can write information back as data.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"To give a concrete example, Fisher's famous iris data set can exist on-disk as a CSV, comprised of bytes or ASCII characters. This is the data form. If we want to do useful analysis, we would want to transform the data into say a table of information (e.g. as a DataFrame). This is the information form. We can load the information form by parsing the CSV data, and write the information back by serialising the table.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"There are thus three essential transformations that can occur involving a data set:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The transformation of a specification into a source of data, termed storage\nThe transformation of data into information, termed a loader\nThe transformation of information into data, termed a writer","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"#Transformers,-Data-Sets,-and-Data-Collections","page":"Introduction","title":"Transformers, Data Sets, and Data Collections","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Each DataSet can have any number of storage, loader, and writer transformers. All DataSets must be part of a DataCollection which essentially provides a context for the existence of a particular data set (e.g. you might store Fisher's iris data under a \"Flower measurements\" data collection). The DataCollections loaded at any one time form the DataCollection stack. The stack essentially acts as a load-path, if you just ask for the iris data set, it will be fetched from the top collection on the stack that can satisfy it.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"It is also worth noting that \"child\" elements of this structure (data sets and transformers) contain a link back to their parent, and so from any part of a DataCollection the whole can be accessed.","category":"page"},{"location":"#Extreme-extensibility","page":"Introduction","title":"Extreme extensibility","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The plethora of formats, workflows, and tools that surround data make designing a \"do it all\" system implausible. A much easier task is to produce a system that can be adapted to serve as many use cases as possible, even ones the designers have never conceived of! To that end, extensibility is weaved throughout the code base. The core system (DataToolkitBase) is generic to the point of being useless on its own, and special-case behaviour has been avoided. Many basic features (such as default values) are implemented as plugins, to avoid being inadvertently privileged in the core system.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"I have yet to come across a feature that could not be implemented under this framework.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"EditURL=\"tutorial.org\"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this tutorial you will be guided through some of the main usage patterns involving DataToolkit. After doing the first step (Initialising a Data Collection), all other sections can be treated as self-contained exercises.","category":"page"},{"location":"tutorial/#Initialising-a-Data-Collection","page":"Tutorial","title":"Initialising a Data Collection","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"First, we will create a new environment to run through the tutorial in, and load the DataToolkit package.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> using Pkg\n\njulia> expanduser(\"~/Documents/datatoolkit_tutorial\") |> mkpath |> cd\n\njulia> Pkg.activate(\".\")\n  Activating new project at `~/Documents/datatoolkit_tutorial`\n\njulia> Pkg.add(\"DataToolkit\")\n   Resolving package versions...\n    Updating `~/Documents/datatoolkit_tutorial/Project.toml`\n  [dc83c90b] + DataToolkit\n  ...\nPrecompiling project...\n\njulia> using DataToolkit","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notice that by typing } at an empty julia> prompt, the REPL prompt will change to (⋅) data> (in the same way that typing ] enters the pkg> REPL). This is the \"Data REPL\", and the (⋅) prefix indicates the current project. When there is no current project, the dot is shown.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In the data REPL, we can see a list of all the available commands by typing help or ?, which will pull up a command list like so:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(⋅) data> help\n Command  Action\n ────────────────────────────────────────────────────────\n <cmd>    <brief description of what cmd does>\n ...      ...\n help     Display help text for commands and transformers","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"tip: Tip\nGet more information on a particular command with help <cmd>, you can even get more information on what help does with help help 😉.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We will initialise a new data collection with the init command.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nWe can use the full command (init), or any substring that uniquely identifies the command (e.g. it).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(⋅) data> init\n Create Data.toml for current project? [Y/n]: y\n Name: tutorial\n Use checksums by default? [Y/n]: n\n ✓ Created new data collection 'tutorial' at /home/tec/Documents/datatoolkit_tutorial/Data.toml","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"tip: Tip\nThere are a few other ways init can be used, see the full docs with help init.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If we look at the ~/Documents/datatoolkit_tutorial folder, we should now see three files.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"shell> tree\n.\n├── Data.toml\n├── Manifest.toml\n└── Project.toml","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Looking inside the Data.toml, we can see what a data collection with no data sets looks like:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"data_config_version = 0\nuuid = \"f20a77d0-0dc9-41bb-875b-ad0bf42c90bd\"\nname = \"tutorial\"\nplugins = [\"store\", \"defaults\", \"memorise\"]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nThe plugins store, defaults, and memorise are the default set of plugins, which is why we see them here. A minimal Data.toml would have plugins = [].","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"At this point, we have created a new data collection, and seen what is created. If we close the Julia session and re-open a REPL in the datatoolkit_test project, loading DataToolkit will automatically cause the data collection we just created to be loaded as well, as seen in the prompt prefix.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> using DataToolkit\n\n(tutorial) data> # after typing '}'","category":"page"},{"location":"tutorial/#Adding-and-loading-the-Iris-data-set","page":"Tutorial","title":"Adding and loading the Iris data set","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now we have a data collection, we can add data sets too it. Fisher's Iris data set is part of the scikit-learn repository, which makes it fairly easy to find a link to it: https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can easily add this as a DataSet using the add Data REPL command,","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> add iris https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\n Description: Fisher's famous Iris flower measurements\n ✓ Created 'iris' (3f3d7714-22aa-4555-a950-78f43b74b81c)\n DataSet tutorial:iris\n  Storage: web(IO, Vector{UInt8}, String, FilePath)\n  Loaders: csv(DataFrame, Matrix, File)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nSay halfway through we decide we don't want to proceed with this Data REPL command, at any point we can interrupt it with ^C (Control + C) and abort the action. This works with other Data REPL commands in the same way.(tutorial) data> add iris https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\n Description:  ! Aborted","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The add command tries to be a bit clever and guess how the data should be acquired and loaded. In this case it (correctly) guessed that this file should be downloaded from the web, and loaded as a CSV. It is worth noting that downloading will occur when iris is first accessed or the store fetch Data REPL command is run.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The DataSet tutorial:iris and Storage:​/​Loaders: lines are how all DataSet​s are displayed. Using the dataset function we can obtain any data set easily by name, and so dataset(\"iris\") will show the same information.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> dataset(\"iris\")\nDataSet tutorial:iris\n  Storage: web(IO, Vector{UInt8}, String, FilePath)\n  Loaders: csv(DataFrame, Matrix, File)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can see from the Storage: web(IO, Vector{UInt8}, String, FilePath) line that the web storage driver is being used, and it can make the content available as an IO, Vector{UInt8}, String, or FilePath (a string wrapper type provided by DataToolkitBase for dispatch purposes). Similarly, the Loaders: csv(DataFrame, Matrix, File) tells us that the csv loader is being used, and it can provide a DataFrame, Matrix, or CSV.File.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If we look at the Data.toml again, we can see how the iris data set is represented:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To obtain a particular loaded form of the data set, we can use the read function. For instance, read(dataset(\"iris\"), DataFrame) or read(dataset(\"iris\"), Matrix). We can also omit the second argument, in which case the first form that can be loaded will be (e.g. in this case since DataFrames is not loaded, iris can not be loaded as a DataFrame, but it can be loaded as a Matrix, and so it will be).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> read(dataset(\"iris\"))\n[ Info: Lazy-loading KangarooTwelve [2a5dabf5-6a39-42aa-818d-ce8a58d1b312]\n │ Package KangarooTwelve not found, but a package named KangarooTwelve is available from a registry.\n │ Install package?\n │   (dt_test) pkg> add KangarooTwelve\n └ (y/n/o) [y]: y\n    Updating registry at `~/.julia/registries/General.toml`\n   Resolving package versions...\n    Updating `/tmp/dt_test/Project.toml`\n  [2a5dabf5] + KangarooTwelve v1.0.0\n    Updating `/tmp/dt_test/Manifest.toml`\n    ...\n[ Info: Lazy-loading KangarooTwelve [2a5dabf5-6a39-42aa-818d-ce8a58d1b312]\n[ Info: Lazy-loading CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n │ Package CSV not found, but a package named CSV is available from a registry.\n │ Install package?\n │   (dt_test) pkg> add CSV\n └ (y/n/o) [y]:\n   Resolving package versions...\n    Updating `/tmp/dt_test/Project.toml`\n  [336ed68f] + CSV v0.10.11\n    Updating `/tmp/dt_test/Manifest.toml`\n    ...\n[ Info: Lazy-loading CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n150×5 Matrix{Float64}:\n 5.1  3.5  1.4  0.2  0.0\n 4.9  3.0  1.4  0.2  0.0\n 4.7  3.2  1.3  0.2  0.0\n ⋮\n 6.5  3.0  5.2  2.0  2.0\n 6.2  3.4  5.4  2.3  2.0\n 5.9  3.0  5.1  1.8  2.0","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nWe haven't installed the KangarooTwelve (a cryptographic hash) or CSV packages, but thanks to the lazy-loading system we are presented with the option to install them on-the-fly. The KangarooTwelve package is only used when hashing new data, or verifying the hash of downloaded data. Should you want to avoid lazy-loading, you can always just load the CSV package yourself before trying to access information that uses the csv loader.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Because read(dataset(\"iris\")) is a fairly common pattern, for convenience there is a d\"\" \"data set in loaded form\" macro. d\"iris\" is equivalent to read(dataset(\"iris\")).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Having the iris data as a Matrix is fine, but it would be nicer to have it as a DataFrame. Since that is the first format listed, if we just install DataFrames and ask for iris again (but this time using the d\"\" macro).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> using DataFrames\n │ Package DataFrames not found, but a package named DataFrames is available from a registry.\n │ Install package?\n │   (datatoolkit_tutorial) pkg> add DataFrames\n └ (y/n/o) [y]:\n   Resolving package versions...\n    Updating `~/Documents/datatoolkit_tutorial/Project.toml`\n  [a93c6f00] + DataFrames v1.6.1\n  ...\n  1 dependency successfully precompiled in 25 seconds. 41 already precompiled.\n\njulia> d\"iris\"\n150×5 DataFrame\n Row │ 150      4        setosa   versicolor  virginica\n     │ Float64  Float64  Float64  Float64     Int64\n─────┼──────────────────────────────────────────────────\n   1 │     5.1      3.5      1.4         0.2          0\n   2 │     4.9      3.0      1.4         0.2          0\n   3 │     4.7      3.2      1.3         0.2          0\n  ⋮  │    ⋮        ⋮        ⋮         ⋮           ⋮\n 149 │     6.2      3.4      5.4         2.3          2\n 150 │     5.9      3.0      5.1         1.8          2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"That's nicer, but wait, those column names aren't right! The first line appears to be describing the size of the data (150×4) and the three category names, when the columns should be:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"sepal_length,\nsepal_width,\npetal_length,\npetal_width, and\nspecies_class","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Perhaps there's a way we can specify the correct column names? We could check the online docs for the CSV loader, but we can also look at them with the help Data REPL command.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> help :csv\n  Parse and serialize CSV data\n\n  ...\n\n  Parameters\n  ≡≡≡≡≡≡≡≡≡≡≡≡\n\n    •  args: keyword arguments to be provided to CSV.File, see\n       https://csv.juliadata.org/stable/reading.html#CSV.File.\n\n  As a quick-reference, some arguments of particular interest are:\n\n    •  header: Either,\n       • the row number to parse for column names\n       • the list of column names\n\n  ...","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Perfect! Looks like we can just set the args.header parameter of the csv loader, and we'll get the right column names. To easily do so, we can make use of the edit Data REPL command, which opens up a TOML file with just a single data set in $JULIA_EDITOR (which defaults to $VISUAL​/​$EDITOR) and records the changes upon exit.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> edit iris","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Setting args.header is as simple as editing the iris loader to the following value (adding one line):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[[iris.loader]]\ndriver = \"csv\"\nargs.header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"After saving and exiting, you'll be presented with a summary of the changes and a prompt to accept them.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> edit iris\n ~ Modified loader:\n   ~ Modified [1]:\n     + Added args\n Does this look correct? [y/N]: y\n ✓ Edited 'iris' (3f3d7714-22aa-4555-a950-78f43b74b81c)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now if we ask for the iris data set again, we should see the correct headers.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> d\"iris\"\n151×5 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width  species_class\n     │ Float64       Float64      String7       String15     String15\n─────┼─────────────────────────────────────────────────────────────────────\n   1 │        150.0          4.0  setosa        versicolor   virginica\n   2 │          5.1          3.5  1.4           0.2          0\n   3 │          4.9          3.0  1.4           0.2          0\n  ⋮  │      ⋮             ⋮            ⋮             ⋮             ⋮\n 150 │          6.2          3.4  5.4           2.3          2\n 151 │          5.9          3.0  5.1           1.8          2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The headers are correct, but now the first line is counted as part of the data. This can be fixed by editing iris again and setting args.skipto to 2 in the csv loader settings.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The final iris entry in the Data.toml should look like so:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    checksum = \"k12:cfb9a6a302f58e5a9b0c815bb7e8efb4\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"\n\n        [iris.loader.args]\n        header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]\n        skipto = 2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now, you have a Project.toml, Manifest.toml, and Data.toml that can be relocated to other systems and d\"iris\" will consistently produce the exact same DataFrame.","category":"page"},{"location":"tutorial/#On-ensuring-the-integrity-of-the-downloaded-data","page":"Tutorial","title":"On ensuring the integrity of the downloaded data","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"One of the three plugins used by default is the store plugin. It is responsible for caching IO data and checking data validity. For a more complete description of what it does, see the web docs or the Data REPL (sub)command plugin info store.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"There are two immediate impacts of this plugin we can easily observe. The first is that we can load the iris data set offline in a fresh Julia session, and in fact if we copy the iris specification into a separate data set it will re-use the same downloaded data.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The second, is that by setting iris's web storage driver's checksum property to \"auto\" (as is done by default), the next time we load iris a checksum will be generated and saved. If in future the web storage driver produces different data, this will now be caught and raised. This can be done automatically by setting the default value to \"auto\", which we were prompted to do during initialisation.","category":"page"},{"location":"tutorial/#Multi-step-analysis-with-the-Boston-Housing-data-set","page":"Tutorial","title":"Multi-step analysis with the Boston Housing data set","text":"","category":"section"},{"location":"tutorial/#Loading-the-data","page":"Tutorial","title":"Loading the data","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Boston Housing data set is part of the RDatasets package, and we can obtain a link to the raw data file in the repository: https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As with the Iris data, we will use the add Data REPL command to conveniently create a new data set.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> add boston https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz\n Description: The Boston Housing data set. This contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\n ✓ Created 'boston' (02968c42-828e-4f22-86b8-ec67ac629a03)\n DataSet tutorial:boston\n  Storage: web(IO, Vector{UInt8}, String, FilePath)\n  Loaders: chain(DataFrame, Matrix, File)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This example is a bit more complicated because we have a gzipped CSV. There is a gzip-decompressing loader, and a CSV loader, but no single loader that does both. Thankfully, there is a special loader called chain that allows for multiple loaders to be chained together. We can see it's automatically been used here, and if we inspect the Data.toml we an see the following generated representation of the boston housing data, in which the gzip and csv loaders are both used.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[[boston]]\nuuid = \"02968c42-828e-4f22-86b8-ec67ac629a03\"\ndescription = \"The Boston Housing data set. This contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\"\n\n    [[boston.storage]]\n    driver = \"web\"\n    url = \"https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz\"\n\n    [[boston.loader]]\n    driver = \"chain\"\n    loaders = [\"gzip\", \"csv\"]\n    type = [\"DataFrame\", \"Matrix\", \"CSV.File\"]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nWe can see the loaders chain passes the data through are given by loaders = [\"gzip\", \"csv\"]. For more information on the chain loader see help :chain in the Data REPL or the online documentation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Thanks to this cleverness, obtaining the Boston Housing data as a nice DataFrame is as simple as d\"boston\" (when DataFrames is loaded).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> d\"boston\"\n506×14 DataFrame\n Row │ Crim     Zn       Indus    Chas   NOx      Rm       Age      Dis      Rad   ⋯\n     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64 ⋯\n─────┼──────────────────────────────────────────────────────────────────────────────\n   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1 ⋯\n   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2\n   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2\n  ⋮  │    ⋮        ⋮        ⋮       ⋮       ⋮        ⋮        ⋮        ⋮       ⋮   ⋱\n 504 │ 0.06076      0.0    11.93      0    0.573    6.976     91.0   2.1675      1\n 505 │ 0.10959      0.0    11.93      0    0.573    6.794     89.3   2.3889      1\n 506 │ 0.04741      0.0    11.93      0    0.573    6.03      80.8   2.505       1 ⋯","category":"page"},{"location":"tutorial/#Cleaning-the-data","page":"Tutorial","title":"Cleaning the data","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Say the data needs some massaging, such as imputation, outlier removal, or restructuring. We can cleanly handle this by creating a second dataset that uses the value of the initial dataset. Say we consider this initial data unclean, and that to \"clean\" this dataset we filter out the entries where the we only keep entries where the MedV value is within the 90% quantile. We can easily do this with the make Data REPL command.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For this, we'll want to use the StatsBase package, so we'll add it and then make it available to use with DataToolkit.@addpkgs.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(datatoolkit_tutorial) pkg> add StatsBase\n\njulia> DataToolkit.@addpkgs StatsBase","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"info: Info\nThe DataToolkit.@addpkgs StatsBase line will need to be executed in every fresh Julia session, when creating a data package it makes sense to put this within the __init__ function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now we can create the boston (clean) dataset with the make command.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> make boston (clean)\n\n(data) julia> @require StatsBase\nStatsBase\n\n(data) julia> proportion = 0.8\n0.8\n\n(data) julia> column = \"MedV\"\n\n(data) julia> vals = d\"boston\"[!, column]\n506-element Vector{Float64}:\n\n(data) julia> minval, maxval = StatsBase.quantile(vals, [0.5 - proportion/2, 0.5 + proportion/2])\n2-element Vector{Float64}:\n 12.75\n 34.8\n\n(data) julia> mask = minval .<= vals .<= maxval\n506-element BitVector:\n\n(data) julia> d\"boston\"[mask, :]\n456×14 DataFrame...\n\n^D\n\n Would you like to edit the final script? [Y/n]: n\n What is the type of the returned value? DataFrame\n Description: Cleaned Boston Housing data\n Should the script be inserted inline (i), or as a file (f)? i\n ✓ Created 'boston (clean)' (5162814a-120f-4cdc-9958-620189295330)\n\n(tutorial) data>","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can look inside the Data.toml to see the new entry.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[[\"boston (clean)\"]]\nuuid = \"5162814a-120f-4cdc-9958-620189295330\"\ndescription = \"Cleaned Boston Housing data\"\n\n    [[\"boston (clean)\".loader]]\n    driver = \"julia\"\n    function = '''\nfunction (; var\"data#boston\")\n    @require StatsBase\n    proportion = 0.8\n    column = \"MedV\"\n    vals = var\"data#boston\"[!, column]\n    (minval, maxval) = StatsBase.quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\n    mask = minval .<= vals .<= maxval\n    var\"data#boston\"[mask, :]\nend\n'''\n    type = \"DataFrame\"\n\n        [\"boston (clean)\".loader.arguments]\n        \"data#boston\" = \"📇DATASET<<boston::DataFrame>>\"","category":"page"},{"location":"tutorial/#Fitting-a-linear-model","page":"Tutorial","title":"Fitting a linear model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now let's say we want to fit a linear model for the relationship between MedV and Rm. We could do this in a script … or create another derived dataset.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's do this with GLM, so first run ] add GLM, then DataToolkit.@addpkgs GLM. Now we'll create another derived data set with make.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> make boston Rm ~ MedV\n\n(data) julia> @require GLM\n\n(data) julia> GLM.lm(GLM.@formula(Rm ~ MedV), d\"boston (clean)\")\n\n^D\n\n Would you like to edit the final script? [Y/n]: n\n What is the type of the returned value? Any\n Description: A linear model for the relation between Rm and MedV\n Should the script be inserted inline (i), or as a file (f)? i\n ✓ Created 'boston Rm ~ MedV' (e720acb2-5ed1-417f-bfd0-668c21134c87)\n\n(tutorial) data>","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"info: Info\nFor now, manually specify Any as the return type instead of the default StatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.CholeskyPivoted{Float64,Array{Float64,2},Array{Int64,1}}}},Array{Float64,2}}. It's currently difficult for DataToolkitBase to represent types that rely on nested modules, which occurs here.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Obtaining the linear regression result is as easy as fetching any other dataset.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> d\"boston Rm ~ MedV\"\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nRm ~ 1 + MedV\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  4.95241     0.0753342  65.74    <1e-99  4.80436    5.10045\nMedV         0.0588453   0.0033047  17.81    <1e-53  0.0523509  0.0653397\n─────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"tutorial/#A-more-easily-tunable-cleaner","page":"Tutorial","title":"A more easily tunable cleaner","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In the current implementation of boston (clean), we hardcoded a proportion value of 0.8, and set the column to \"MedV\". It could be nice if we made those more easily tunable. We can do this by turning them into keyword arguments of the function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To make this change, we will use the edit Data REPL command.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> edit boston (clean)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This will open up a temporary TOML file containing the boston (clean) dataset in your text editor of choice. In this file, change the function to:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function (; var\"data#boston\", proportion, column)\n    @require StatsBase\n    vals = var\"data#boston\"[!, column]\n    (minval, maxval) = StatsBase.quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\n    mask = minval .<= vals .<= maxval\n    var\"data#boston\"[mask, :]\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We will then move the proportion = 0.8 and column = \"MedV\" lines to the arguments table.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[\"boston (clean)\".loader.arguments]\n\"data#boston\" = \"📇DATASET<<boston::DataFrame>>\"\nproportion = 0.8\ncolumn = \"MedV\"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Aftre making these changes and closing the file, we'll be asked if we want to make this change (we do).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> edit boston (clean)\n ~ Modified loader:\n   ~ Modified [1]:\n     ~ Modified arguments:\n       + Added column\n       + Added proportion\n     ~ Modified function:\n       \"function (; var\\\"data#boston\\\")\\n    @require StatsBase\\n    proportion = 0.9\\n    column = \\\"MedV\\\"\\n    vals = var\\\"data#boston\\\"[!, column]\\n    (minval, maxval) = StatsBase.quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\\n    mask = minval .<= vals .<= maxval\\n    var\\\"data#boston\\\"[mask, :]\\nend\\n\" ~> \"function (; var\\\"data#boston\\\", proportion, column)\\n    @require StatsBase\\n    vals = var\\\"data#boston\\\"[!, column]\\n    (minval, maxval) = StatsBase.quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\\n    mask = minval .<= vals .<= maxval\\n    var\\\"data#boston\\\"[mask, :]\\nend\\n\"\n Does this look correct? [y/N]: y\n ✓ Edited 'boston (clean)' (5162814a-120f-4cdc-9958-620189295330)","category":"page"},{"location":"tutorial/#Propagating-changes","page":"Tutorial","title":"Propagating changes","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With our new parameterisation of the cleaning step, we can now easily tune the cleaning step. We can see the results of this propagating through in the boston Rm ~ MedV dataset.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"First, see that the d\"boston Rm ~ MedV\" result is the same as it was before.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> d\"boston Rm ~ MedV\"\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nRm ~ 1 + MedV\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  4.98609     0.0978742  50.94    <1e-99  4.79369    5.1785\nMedV         0.0563562   0.0044222  12.74    <1e-30  0.0476627  0.0650497\n─────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now, edit the boston (clean) dataset again and change the proportion to 0.95.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(tutorial) data> edit boston (clean)\n ~ Modified loader:\n   ~ Modified [1]:\n     ~ Modified arguments:\n       ~ Modified proportion:\n         0.8 ~> 0.95\n Does this look correct? [y/N]: y\n ✓ Edited 'boston (clean)' (5162814a-120f-4cdc-9958-620189295330)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since boston (clean) is an input of boston Rm ~ MedV, and all inputs are recursively hashed (like in a Merkle tree), we can immediately see the (small) change simply by fetching it again — it is automatically recomputed.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> d\"boston Rm ~ MedV\"\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nRm ~ 1 + MedV\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  5.05849    0.0618848   81.74    <1e-99   4.9369    5.18008\nMedV         0.0541727  0.00251511  21.54    <1e-72   0.049231  0.0591144\n─────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"tutorial/#The-final-Data.toml","page":"Tutorial","title":"The final Data.toml","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"At the end of this tutorial (or should you wish to just poke at the results), you should end up with a Data.toml that looks like this:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"data_config_version = 0\nuuid = \"f20a77d0-0dc9-41bb-875b-ad0bf42c90bd\"\nname = \"tutorial\"\nplugins = [\"defaults\", \"store\"]\n\n[config.defaults.storage._]\nchecksum = \"auto\"\n\n[[boston]]\nuuid = \"02968c42-828e-4f22-86b8-ec67ac629a03\"\ndescription = \"The Boston Housing data set. This contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\"\n\n    [[boston.storage]]\n    driver = \"web\"\n    checksum = \"k12:663371e9040b883267104b32d8ac28e6\"\n    url = \"https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz\"\n\n    [[boston.loader]]\n    driver = \"chain\"\n    loaders = [\"gzip\", \"csv\"]\n\n[[\"boston (clean)\"]]\nuuid = \"5162814a-120f-4cdc-9958-620189295330\"\ndescription = \"Cleaned Boston Housing data\"\n\n    [[\"boston (clean)\".loader]]\n    driver = \"julia\"\n    function = '''\nfunction (; var\"data#boston\", proportion, column)\n    @require StatsBase\n    vals = var\"data#boston\"[!, column]\n    (minval, maxval) = StatsBase.quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\n    mask = minval .<= vals .<= maxval\n    var\"data#boston\"[mask, :]\nend\n'''\n    type = \"DataFrame\"\n\n        [\"boston (clean)\".loader.arguments]\n        column = \"MedV\"\n        \"data#boston\" = \"📇DATASET<<boston::DataFrame>>\"\n        proportion = 0.95\n\n[[\"boston Rm ~ MedV\"]]\nuuid = \"e720acb2-5ed1-417f-bfd0-668c21134c87\"\ndescription = \"A linear model for the relation between Rm and MedV\"\n\n    [[\"boston Rm ~ MedV\".loader]]\n    driver = \"julia\"\n    function = \"\"\"\nfunction (; var\\\"data#boston (clean)\\\")\n    @require GLM\n    GLM.lm(GLM.@formula(Rm ~ MedV), var\\\"data#boston (clean)\\\")\nend\n\"\"\"\n\n        [\"boston Rm ~ MedV\".loader.arguments]\n        \"data#boston (clean)\" = \"📇DATASET<<boston (clean)::DataFrame>>\"\n\n[[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    checksum = \"k12:cfb9a6a302f58e5a9b0c815bb7e8efb4\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"\n\n        [iris.loader.args]\n        header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]\n        skipto = 2","category":"page"}]
}
