var documenterSearchIndex = {"docs":
[{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"EditURL=\"advising.org\"","category":"page"},{"location":"advising/#Data-Advising","page":"Data Advice","title":"Data Advising","text":"","category":"section"},{"location":"advising/#Advice","page":"Data Advice","title":"Advice","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Inspired by Lisp, DataToolkitCore comes with a method of completely transforming its behaviour at certain defined points. This is essentially a restricted form of Aspect-oriented programming. At certain declared locations (termed \"join points\"), we consult a list of \"advise\" functions that modify the execution at that point, and apply the (matched via \"pointcuts\") advise functions accordingly.","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"(Image: image)","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Each applied advise function is wrapped around the invocation of the join point, and is able to modify the arguments, execution, and results of the join point.","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"(Image: image)","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Advice","category":"page"},{"location":"advising/#DataToolkitCore.Advice","page":"Data Advice","title":"DataToolkitCore.Advice","text":"Advice{func, context} <: Function\n\nAdvices allow for composable, highly flexible modifications of data by encapsulating a function call. They are inspired by elisp's advice system, namely the most versatile form ‚Äî :around advice, and Clojure's advisors.\n\nA Advice is essentially a function wrapper, with a priority::Int attribute. The wrapped functions should be of the form:\n\n(action::Function, args...; kargs...) ->\n  ([post::Function], action::Function, args::Tuple, [kargs::NamedTuple])\n\nShort-hand return values with post or kargs omitted are also accepted, in which case default values (the identity function and (;) respectively) will be automatically substituted in.\n\n    input=(action args kwargs)\n         ‚îÉ                 ‚îè‚ï∏post=identity\n       ‚ï≠‚îÄ‚ïÇ‚îÄ‚îÄ‚îÄ‚îÄadvisor 1‚îÄ‚îÄ‚îÄ‚îÄ‚ïÇ‚îÄ‚ïÆ\n       ‚ï∞‚îÄ‚ïÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÇ‚îÄ‚ïØ\n       ‚ï≠‚îÄ‚ïÇ‚îÄ‚îÄ‚îÄ‚îÄadvisor 2‚îÄ‚îÄ‚îÄ‚îÄ‚ïÇ‚îÄ‚ïÆ\n       ‚ï∞‚îÄ‚ïÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÇ‚îÄ‚ïØ\n       ‚ï≠‚îÄ‚ïÇ‚îÄ‚îÄ‚îÄ‚îÄadvisor 3‚îÄ‚îÄ‚îÄ‚îÄ‚ïÇ‚îÄ‚ïÆ\n       ‚ï∞‚îÄ‚ïÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÇ‚îÄ‚ïØ\n         ‚îÉ                 ‚îÉ\n         ‚ñº                 ‚ñΩ\naction(args; kargs) ‚îÅ‚îÅ‚îÅ‚îÅ‚ñ∂ post‚ï∫‚îÅ‚îÅ‚ñ∂ result\n\nTo specify which transforms a Advice should be applied to, ensure you add the relevant type parameters to your transducing function. In cases where the transducing function is not applicable, the Advice will simply act as the identity function.\n\nAfter all applicable Advices have been applied, action(args...; kargs...) |> post is called to produce the final result.\n\nThe final post function is created by rightwards-composition with every post entry of the advice forms (i.e. at each stage post = post ‚àò extra is run).\n\nThe overall behaviour can be thought of as shells of advice.\n\n        ‚ï≠‚ïå advisor 1 ‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îÄ‚ïÆ\n        ‚îÜ ‚ï≠‚ïå advisor 2 ‚ïå‚ïå‚ïå‚ïå‚ïå‚ïÆ ‚îÜ\n        ‚îÜ ‚îÜ                 ‚îÜ ‚îÜ\ninput ‚îÅ‚îÅ‚îø‚îÅ‚îø‚îÅ‚îÅ‚îÅ‚ñ∂ function ‚îÅ‚îÅ‚îÅ‚îø‚îÅ‚îø‚îÅ‚îÅ‚ñ∂ result\n        ‚îÜ ‚ï∞‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïØ ‚îÜ\n        ‚ï∞‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïØ\n\nConstructors\n\nAdvice(priority::Int, f::Function)\nAdvice(f::Function) # priority is set to 1\n\nExamples\n\n1. Logging every time a DataSet is loaded.\n\nloggingadvisor = Advice(\n    function(post::Function, f::typeof(load), loader::DataLoader, input, outtype)\n        @info \"Loading $(loader.data.name)\"\n        (post, f, (loader, input, outtype))\n    end)\n\n2. Automatically committing each data file write.\n\nwritecommitadvisor = Advice(\n    function(post::Function, f::typeof(write), writer::DataWriter{:filesystem}, output, info)\n        function writecommit(result)\n            run(`git add $output`)\n            run(`git commit -m \"update $output\"`)\n            result\n        end\n        (post ‚àò writecommit, writefn, (output, info))\n    end)\n\n\n\n\n\n","category":"type"},{"location":"advising/#Advisement-points-(standard-join-points)","page":"Data Advice","title":"Advisement points (standard join points)","text":"","category":"section"},{"location":"advising/#Parsing-and-serialisation-of-data-sets-and-collections","page":"Data Advice","title":"Parsing and serialisation of data sets and collections","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"DataCollection‚Äãs, DataSet‚Äãs, and AbstractDataTransformer‚Äãs are advised at two stages during parsing:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"When calling fromspec on the Dict representation, at the start of parsing\nAt the end of the fromspec function, calling identity on the object","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Serialisation is performed through the tospec call, which is also advised.","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"The signatures of the advised function calls are as follows:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"fromspec(DataCollection, spec::Dict{String, Any}; path::Union{String, Nothing})::DataCollection\nidentity(collection::DataCollection)::DataCollection\ntospec(collection::DataCollection)::Dict","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"fromspec(DataSet, collection::DataCollection, name::String, spec::Dict{String, Any})::DataSet\nidentity(dataset::DataSet)::DataSet\ntospec(dataset::DataSet)::Dict","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"fromspec(ADT::Type{<:AbstractDataTransformer}, dataset::DataSet, spec::Dict{String, Any})::ADT\nidentity(adt::AbstractDataTransformer)::AbstractDataTransformer\ntospec(adt::AbstractDataTransformer)::Dict","category":"page"},{"location":"advising/#Processing-identifiers","page":"Data Advice","title":"Processing identifiers","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"Both the parsing of an Identifier from a string, and the serialisation of an Identifier to a string are advised. Specifically, the following function calls:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"parse_ident(spec::AbstractString)\nstring(ident::Identifier)","category":"page"},{"location":"advising/#The-data-flow-arrows","page":"Data Advice","title":"The data flow arrows","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"The reading, writing, and storage of data may all be advised. Specifically, the following function calls:","category":"page"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"load(loader::DataLoader, datahandle, as::Type)\nstorage(provider::DataStorage, as::Type; write::Bool)\nsave(writer::DataWriter, datahandle, info)","category":"page"},{"location":"advising/#Index-of-advised-calls-(all-known-join-points)","page":"Data Advice","title":"Index of advised calls (all known join points)","text":"","category":"section"},{"location":"advising/","page":"Data Advice","title":"Data Advice","text":"using Markdown\ncontent = Any[]\n\nconst AdviseRecord = NamedTuple{(:location, :parent, :invocation), Tuple{LineNumberNode, <:Union{Expr, Symbol}, Expr}}\nfunction findadvice!(acc::Vector{AdviseRecord}, expr::Expr; parent=nothing)\n    if expr.head == :macrocall && first(expr.args) == Symbol(\"@advise\")\n        !isnothing(parent) || @warn \"Macro @$(expr.args[2]) has no parent function\"\n        push!(acc, (; location=expr.args[2], parent, invocation=expr.args[end]))\n    else\n        if isnothing(parent) && expr.head == :function\n            parent = if first(expr.args) isa Expr\n                first(first(expr.args).args)\n            else\n                first(expr.args)\n            end\n        elseif isnothing(parent) && expr.head == :(=) &&\n            first(expr.args) isa Expr && first(expr.args).head == :call\n            parent = first(first(expr.args).args)\n        end\n        findadvice!.(Ref(acc), expr.args; parent)\n    end\nend\nfindadvice!(acc, ::Any; parent=nothing) = nothing\n\nalladvice = Vector{AdviseRecord}()\nfor (root, dirs, files) in walkdir(\"../../src\")\n    for file in files\n        file == \"precompile.jl\" && continue\n        @info \"Analysing $file for advise\"\n        path = joinpath(root, file)\n        expr = Meta.parseall(read(path, String); filename=path)\n        findadvice!(alladvice, expr)\n    end\nend\n\nAdvItem = NamedTuple{(:line, :parent, :invocation), Tuple{Int, Union{Expr, Symbol}, Expr}}\nadvbyfunc = Dict{Symbol, Dict{Symbol, Vector{AdvItem}}}()\natypes = first.(getfield.(getfield.(alladvice, :invocation), :args)) |> unique\nafiles = getfield.(getfield.(alladvice, :location), :file) |> unique\n\nfor atype in atypes\n    advs = filter(a -> first(a.invocation.args) == atype, alladvice)\n    advbyfunc[atype] = Dict{Symbol, Vector{AdvItem}}()\n    for (; location, parent, invocation) in advs\n        if !haskey(advbyfunc[atype], location.file)\n            advbyfunc[atype][location.file] = Vector{AdvItem}()\n        end\n        push!(advbyfunc[atype][location.file], (; line=location.line, parent, invocation))\n    end\nend\n\npush!(content, Markdown.Paragraph([\n    \"There are \", Markdown.Bold(string(length(alladvice))),\n    \" advised function calls, across \",\n    Markdown.Bold(string(length(unique(getfield.(getfield.(alladvice, :location), :file))))),\n    \" files, covering \", Markdown.Bold(string(length(advbyfunc))),\n    \" functions (automatically detected).\"]))\n\npush!(content, Markdown.Header{3}([\"Arranged by function\"]))\n\nfor fname in sort(keys(advbyfunc) |> collect)\n    instances = advbyfunc[fname]\n    nadv = sum(length, values(instances))\n    push!(content, Markdown.Header{4}([\n        Markdown.Code(String(fname)),\n        if nadv == 1\n            \" (1 instance)\"\n        else\n            \" ($nadv instances)\"\n        end]))\n    list = Markdown.List(Any[], -1, false)\n    for file in sort(keys(instances) |> collect)\n        details = instances[file]\n        sublist = Markdown.List(Any[], -1, false)\n        for (; line, parent, invocation) in details\n            push!(sublist.items, Markdown.Paragraph(\n                [\"On line \", string(line), \" \",\n                 Markdown.Code(string(invocation)),\n                 \" is advised within a \",\n                 Markdown.Code(string(parent)), \" method.\"]))\n        end\n        push!(list.items, Any[\n            Markdown.Paragraph([Markdown.Italic(last(splitpath(String(file))))]),\n            sublist])\n    end\n    push!(content, list)\nend\n\npush!(content, Markdown.Header{3}([\"Arranged by file\"]))\n\nadvbyfile = Dict{Symbol, Vector{AdvItem}}()\nfor (; location, parent, invocation) in alladvice\n    if !haskey(advbyfile, location.file)\n        advbyfile[location.file] = Vector{AdvItem}()\n    end\n    push!(advbyfile[location.file], (; line=location.line, parent, invocation))\nend\n\nfor file in sort(afiles)\n    instances = advbyfile[file]\n    push!(content, Markdown.Header{5}([\n        Markdown.Code(last(splitpath(String(file)))),\n        if length(instances) == 1\n            \" (1 instance)\"\n        else\n            \" ($(length(instances)) instances)\"\n        end]))\n    list = Markdown.List(Any[], -1, false)\n    for (; line, parent, invocation) in instances\n        push!(list.items, [Markdown.Paragraph(\n            [\"On line \", string(line), \" \",\n             Markdown.Code(string(invocation)),\n             \" is advised within a \",\n             Markdown.Code(string(parent)), \" method.\"])])\n    end\n    push!(content, list)\nend\n\nMarkdown.MD(content) |> string |> Markdown.parse","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"EditURL=\"usage.org\"","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#Identifying-a-dataset","page":"Usage","title":"Identifying a dataset","text":"","category":"section"},{"location":"usage/#Reading-datasets","page":"Usage","title":"Reading datasets","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"read","category":"page"},{"location":"usage/#Base.read","page":"Usage","title":"Base.read","text":"read(filename::AbstractString, DataCollection; writer::Union{Function, Nothing})\n\nRead the entire contents of a file as a DataCollection.\n\nThe default value of writer is self -> write(filename, self).\n\n\n\n\n\nread(io::IO, DataCollection; path::Union{String, Nothing}=nothing, mod::Module=Base.Main)\n\nRead the entirety of io, as a DataCollection.\n\n\n\n\n\nread(dataset::DataSet, as::Type)\nread(dataset::DataSet) # as default type\n\nObtain information from dataset in the form of as, with the appropriate loader and storage provider automatically determined.\n\nThis executes this component of the overall data flow:\n\n                 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄloader‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n                 ‚ïµ               ‚ñº\nStorage ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Data          Information\n\nThe loader and storage provider are selected by identifying the highest priority loader that can be satisfied by a storage provider. What this looks like in practice is illustrated in the diagram below.\n\n      read(dataset, Matrix) ‚ü∂ ::Matrix ‚óÄ‚ïÆ\n         ‚ï≠‚îÄ‚îÄ‚îÄ‚ïØ        ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∑‚î¨‚îÄ‚îÄ‚îÄ‚ïØ\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ï∏dataset‚ï∫‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó  ‚îÇ\n‚ïë STORAGE      LOADERS           ‚ïë  ‚îÇ\n‚ïë (‚ü∂ File)‚îÄ‚î¨‚îÄ‚ïÆ (File ‚ü∂ String)   ‚ïë  ‚îÇ\n‚ïë (‚ü∂ IO)   ‚îä ‚ï∞‚îÄ(File ‚ü∂ Matrix)‚îÄ‚î¨‚îÄ‚ï´‚îÄ‚îÄ‚ïØ\n‚ïë (‚ü∂ File)‚îÑ‚ïØ   (IO ‚ü∂ String)   ‚îä ‚ïë\n‚ïë              (IO ‚ü∂ Matrix)‚ïå‚ïå‚ïå‚ïØ ‚ïë\n‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\n  ‚îÄ the load path used\n  ‚îÑ an option not taken\n\nThe types that a DataSet can be loaded as are determined by the loaders, their declared types, and the implemented methods. If a method exists that can load dataset to a subtype of as, it will be used. Methods that produce a type declared in dataset's loaders are preferred.\n\n\n\n\n\n","category":"function"},{"location":"usage/#Writing-datasets","page":"Usage","title":"Writing datasets","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"write","category":"page"},{"location":"usage/#Base.write","page":"Usage","title":"Base.write","text":"write(dataset::DataSet, info::Any)\n\nTODO write docstring\n\n\n\n\n\n","category":"function"},{"location":"usage/#Accessing-the-raw-data","page":"Usage","title":"Accessing the raw data","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"open","category":"page"},{"location":"usage/#Base.open","page":"Usage","title":"Base.open","text":"open(dataset::DataSet, as::Type; write::Bool=false)\n\nObtain the data of dataset in the form of as, with the appropriate storage provider automatically selected.\n\nA write flag is also provided, to help the driver pick a more appropriate form of as.\n\nThis executes this component of the overall data flow:\n\n                 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄloader‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n                 ‚ïµ               ‚ñº\nStorage ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Data          Information\n\n\n\n\n\n","category":"function"},{"location":"errors/","page":"Errors","title":"Errors","text":"EditURL=\"errors.org\"","category":"page"},{"location":"errors/#Errors","page":"Errors","title":"Errors","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"This package tries to minimise the use of generic errors, and maximise the helpfulness of error messages. To that end, a number of new error types are defined.","category":"page"},{"location":"errors/#Identifier-exceptions","page":"Errors","title":"Identifier exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnresolveableIdentifier","category":"page"},{"location":"errors/#DataToolkitCore.UnresolveableIdentifier","page":"Errors","title":"DataToolkitCore.UnresolveableIdentifier","text":"UnresolveableIdentifier{T}(identifier::Union{String, UUID}, [collection::DataCollection])\n\nNo T (optionally from collection) could be found that matches identifier.\n\nExample occurrences\n\njulia> d\"iirs\"\nERROR: UnresolveableIdentifier: \"iirs\" does not match any available data sets\n  Did you perhaps mean to refer to one of these data sets?\n    ‚ñ†:iris (75% match)\nStacktrace: [...]\n\njulia> d\"iris::Int\"\nERROR: UnresolveableIdentifier: \"iris::Int\" does not match any available data sets\n  Without the type restriction, however, the following data sets match:\n    dataset:iris, which is available as a DataFrame, Matrix, CSV.File\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"AmbiguousIdentifier","category":"page"},{"location":"errors/#DataToolkitCore.AmbiguousIdentifier","page":"Errors","title":"DataToolkitCore.AmbiguousIdentifier","text":"AmbiguousIdentifier(identifier::Union{String, UUID}, matches::Vector, [collection])\n\nSearching for identifier (optionally within collection), found multiple matches (provided as matches).\n\nExample occurrence\n\njulia> d\"multimatch\"\nERROR: AmbiguousIdentifier: \"multimatch\" matches multiple data sets\n    ‚ñ†:multimatch [45685f5f-e6ff-4418-aaf6-084b847236a8]\n    ‚ñ†:multimatch [92be4bda-55e9-4317-aff4-8d52ee6a5f2c]\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#Package-exceptions","page":"Errors","title":"Package exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnregisteredPackage","category":"page"},{"location":"errors/#DataToolkitCore.UnregisteredPackage","page":"Errors","title":"DataToolkitCore.UnregisteredPackage","text":"UnregisteredPackage(pkg::Symbol, mod::Module)\n\nThe package pkg was asked for within mod, but has not been registered by mod, and so cannot be loaded.\n\nExample occurrence\n\njulia> @require Foo\nERROR: UnregisteredPackage: Foo has not been registered by Main, see @addpkg for more information\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"MissingPackage","category":"page"},{"location":"errors/#DataToolkitCore.MissingPackage","page":"Errors","title":"DataToolkitCore.MissingPackage","text":"MissingPackage(pkg::Base.PkgId)\n\nThe package pkg was asked for, but does not seem to be available in the current environment.\n\nExample occurrence\n\njulia> @addpkg Bar \"00000000-0000-0000-0000-000000000000\"\nBar [00000000-0000-0000-0000-000000000000]\n\njulia> @require Bar\n[ Info: Lazy-loading Bar [00000000-0000-0000-0000-000000000001]\nERROR: MissingPackage: Bar [00000000-0000-0000-0000-000000000001] has been required, but does not seem to be installed.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#Data-Operation-exceptions","page":"Errors","title":"Data Operation exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"CollectionVersionMismatch","category":"page"},{"location":"errors/#DataToolkitCore.CollectionVersionMismatch","page":"Errors","title":"DataToolkitCore.CollectionVersionMismatch","text":"CollectionVersionMismatch(version::Int)\n\nThe version of the collection currently being acted on is not supported by the current version of DataToolkitCore.\n\nExample occurrence\n\njulia> fromspec(DataCollection, Dict{String, Any}(\"data_config_version\" => -1))\nERROR: CollectionVersionMismatch: -1 (specified) ‚â† 0 (current)\n  The data collection specification uses the v-1 data collection format, however\n  the installed DataToolkitCore version expects the v0 version of the format.\n  In the future, conversion facilities may be implemented, for now though you\n  will need to manually upgrade the file to the v0 format.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"EmptyStackError","category":"page"},{"location":"errors/#DataToolkitCore.EmptyStackError","page":"Errors","title":"DataToolkitCore.EmptyStackError","text":"EmptyStackError()\n\nAn attempt was made to perform an operation on a collection within the data stack, but the data stack is empty.\n\nExample occurrence\n\njulia> getlayer(nothing) # with an empty STACK\nERROR: EmptyStackError: The data collection stack is empty\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"ReadonlyCollection","category":"page"},{"location":"errors/#DataToolkitCore.ReadonlyCollection","page":"Errors","title":"DataToolkitCore.ReadonlyCollection","text":"ReadonlyCollection(collection::DataCollection)\n\nModification of collection is not viable, as it is read-only.\n\nExample Occurrence\n\njulia> lockedcollection = DataCollection(Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128)), \"config\" => Dict{String, Any}(\"locked\" => true)))\njulia> write(lockedcollection)\nERROR: ReadonlyCollection: The data collection unnamed#298 is locked\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"TransformerError","category":"page"},{"location":"errors/#DataToolkitCore.TransformerError","page":"Errors","title":"DataToolkitCore.TransformerError","text":"TransformerError(msg::String)\n\nA catch-all for issues involving data transformers, with details given in msg.\n\nExample occurrence\n\njulia> emptydata = DataSet(DataCollection(), \"empty\", Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata)\nERROR: TransformerError: Data set \"empty\" could not be loaded in any form.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnsatisfyableTransformer","category":"page"},{"location":"errors/#DataToolkitCore.UnsatisfyableTransformer","page":"Errors","title":"DataToolkitCore.UnsatisfyableTransformer","text":"UnsatisfyableTransformer{T}(dataset::DataSet, types::Vector{QualifiedType})\n\nA transformer (of type T) that could provide any of types was asked for, but there is no transformer that satisfies this restriction.\n\nExample occurrence\n\njulia> emptydata = DataSet(DataCollection(), \"empty\", Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata, String)\nERROR: UnsatisfyableTransformer: There are no loaders for \"empty\" that can provide a String. The defined loaders are as follows:\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/","page":"Errors","title":"Errors","text":"OrphanDataSet","category":"page"},{"location":"errors/#DataToolkitCore.OrphanDataSet","page":"Errors","title":"DataToolkitCore.OrphanDataSet","text":"OrphanDataSet(dataset::DataSet)\n\nThe data set (dataset) is no longer a child of its parent collection.\n\nThis error should not occur, and is intended as a sanity check should something go quite wrong.\n\n\n\n\n\n","category":"type"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"EditURL=\"newtransformer.org\"","category":"page"},{"location":"newtransformer/#Creating-a-new-data-transformer","page":"Transformer backends","title":"Creating a new data transformer","text":"","category":"section"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"As mentioned before, there are three types of data transformer:","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"storage\nloader\nwriter","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"The three corresponding Julia types are:","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"DataStorage\nDataLoader\nDataWriter","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"All three types accept a driver (symbol) type parameter. For example, a storage transformer using a \"filesystem\" driver would be of the type DataStorage{:filesystem}.","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"Adding support for a new driver is a simple as adding method implementations for the three key data transformer methods:","category":"page"},{"location":"newtransformer/","page":"Transformer backends","title":"Transformer backends","text":"load\nstorage\nsave","category":"page"},{"location":"newtransformer/#DataToolkitCore.load","page":"Transformer backends","title":"DataToolkitCore.load","text":"load(loader::DataLoader{driver}, source::Any, as::Type)\n\nUsing a certain loader, obtain information in the form of as from the data given by source.\n\nThis fulfils this component of the overall data flow:\n\n  ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄloader‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n  ‚ïµ               ‚ñº\nData          Information\n\nWhen the loader produces nothing this is taken to indicate that it was unable to load the data for some reason, and that another loader should be tried if possible. This can be considered a soft failure. Any other value is considered valid information.\n\n\n\n\n\n","category":"function"},{"location":"newtransformer/#DataToolkitCore.storage","page":"Transformer backends","title":"DataToolkitCore.storage","text":"storage(storer::DataStorage, as::Type; write::Bool=false)\n\nFetch a storer in form as, appropiate for reading from or writing to (depending on write).\n\nBy default, this just calls getstorage or putstorage (when write=true).\n\nThis executes this component of the overall data flow:\n\nStorage ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Data\n\n\n\n\n\n","category":"function"},{"location":"newtransformer/#DataToolkitCore.save","page":"Transformer backends","title":"DataToolkitCore.save","text":"save(writer::Datasaveer{driver}, destination::Any, information::Any)\n\nUsing a certain writer, save the information to the destination.\n\nThis fulfils this component of the overall data flow:\n\nData          Information\n  ‚ñ≤               ‚ï∑\n  ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄwriter‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\n\n\n\n\n","category":"function"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"EditURL=\"libinternal.org\"","category":"page"},{"location":"libinternal/#Private-API","page":"Internals","title":"Private API","text":"","category":"section"},{"location":"libinternal/#Abstract-Data-Transformer","page":"Internals","title":"Abstract Data Transformer","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"AbstractDataTransformer","category":"page"},{"location":"libinternal/#DataToolkitCore.AbstractDataTransformer","page":"Internals","title":"DataToolkitCore.AbstractDataTransformer","text":"The supertype for methods producing or consuming data.\n\n                 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄloader‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n                 ‚ïµ               ‚ñº\nStorage ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Data          Information\n                 ‚ñ≤               ‚ï∑\n                 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄwriter‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\nThere are three subtypes:\n\nDataStorage\nDataLoader\nDataWrite\n\nEach subtype takes a Symbol type parameter designating the driver which should be used to perform the data operation. In addition, each subtype has the following fields:\n\ndataset::DataSet, the data set the method operates on\ntype::Vector{<:QualifiedType}, the Julia types the method supports\npriority::Int, the priority with which this method should be used, compared to alternatives. Lower values have higher priority.\nparameters::Dict{String, Any}, any parameters applied to the method.\n\n\n\n\n\n","category":"type"},{"location":"libinternal/#Advice-Amalgamation","page":"Internals","title":"Advice Amalgamation","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"AdviceAmalgamation","category":"page"},{"location":"libinternal/#DataToolkitCore.AdviceAmalgamation","page":"Internals","title":"DataToolkitCore.AdviceAmalgamation","text":"A collection of Advices sourced from available Plugins.\n\nLike individual Advices, a AdviceAmalgamation can be called as a function. However, it also supports the following convenience syntax:\n\n(::AdviceAmalgamation)(f::Function, args...; kargs...) # -> result\n\nConstructors\n\nAdviceAmalgamation(advisors::Vector{Advice}, plugins_wanted::Vector{String}, plugins_used::Vector{String})\nAdviceAmalgamation(plugins::Vector{String})\nAdviceAmalgamation(collection::DataCollection)\n\n\n\n\n\n","category":"type"},{"location":"libinternal/#Qualified-Types","page":"Internals","title":"Qualified Types","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"QualifiedType","category":"page"},{"location":"libinternal/#DataToolkitCore.QualifiedType","page":"Internals","title":"DataToolkitCore.QualifiedType","text":"A representation of a Julia type that does not need the type to be defined in the Julia session, and can be stored as a string. This is done by storing the type name and the module it belongs to as Symbols.\n\nwarning: Warning\nWhile QualifiedType is currently quite capable, it is not currently able to express the full gamut of Julia types. In future this will be improved, but it will likely always be restricted to a certain subset.\n\nSubtyping\n\nWhile the subtype operator cannot work on QualifiedTypes (<: is a built-in), when the Julia types are defined the subset operator ‚äÜ can be used instead. This works by simply converting the QualifiedTypes to the corresponding Type and then applying the subtype operator.\n\njulia> QualifiedTypes(:Base, :Vector) ‚äÜ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> Matrix ‚äÜ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> QualifiedTypes(:Base, :Vector) ‚äÜ AbstractVector\ntrue\n\njulia> QualifiedTypes(:Base, :Foobar) ‚äÜ AbstractVector\nfalse\n\nConstructors\n\nQualifiedType(parentmodule::Symbol, typename::Symbol)\nQualifiedType(t::Type)\n\nParsing\n\nA QualifiedType can be expressed as a string as \"$parentmodule.$typename\". This can be easily parsed as a QualifiedType, e.g. parse(QualifiedType, \"Core.IO\").\n\n\n\n\n\n","category":"type"},{"location":"libinternal/#Global-variables","page":"Internals","title":"Global variables","text":"","category":"section"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"STACK","category":"page"},{"location":"libinternal/#DataToolkitCore.STACK","page":"Internals","title":"DataToolkitCore.STACK","text":"The set of data collections currently available.\n\n\n\n\n\n","category":"constant"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"PLUGINS","category":"page"},{"location":"libinternal/#DataToolkitCore.PLUGINS","page":"Internals","title":"DataToolkitCore.PLUGINS","text":"The set of plugins currently available.\n\n\n\n\n\n","category":"constant"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"DataToolkitCore.EXTRA_PACKAGES","category":"page"},{"location":"libinternal/#DataToolkitCore.EXTRA_PACKAGES","page":"Internals","title":"DataToolkitCore.EXTRA_PACKAGES","text":"The set of packages loaded by each module via @addpkg, for import with @require.\n\nMore specifically, when a module M invokes @addpkg pkg id then EXTRA_PACKAGES[M][pkg] = id is set, and then this information is used with @require to obtain the package from the root module.\n\n\n\n\n\n","category":"constant"},{"location":"libinternal/","page":"Internals","title":"Internals","text":"DATA_CONFIG_RESERVED_ATTRIBUTES","category":"page"},{"location":"libinternal/#DataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES","page":"Internals","title":"DataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES","text":"The data specification TOML format constructs a DataCollection, which itself contains DataSets, comprised of metadata and AbstractDataTransformers.\n\nDataCollection\n‚îú‚îÄ DataSet\n‚îÇ¬† ‚îú‚îÄ AbstractDataTransformer\n‚îÇ¬† ‚îî‚îÄ AbstractDataTransformer\n‚îú‚îÄ DataSet\n‚ãÆ\n\nWithin each scope, there are certain reserved attributes. They are listed in this Dict under the following keys:\n\n:collection for DataCollection\n:dataset for DataSet\n:transformer for AbstractDataTransformer\n\n\n\n\n\n","category":"constant"},{"location":"packages/","page":"Packages","title":"Packages","text":"EditURL=\"packages.org\"","category":"page"},{"location":"packages/#Using-Packages","page":"Packages","title":"Using Packages","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"It is entirely likely that in the course of writing a package providing a custom data transformer, one would come across packages that may be needed.","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"Every possibly desired package could be shoved into the list of dependences, but this is a somewhat crude approach. A more granular approach is enabled with two macros, @addpkg and @require.","category":"page"},{"location":"packages/#Letting-DataToolkitCore-know-about-extra-packages","page":"Packages","title":"Letting DataToolkitCore know about extra packages","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"@addpkg","category":"page"},{"location":"packages/#DataToolkitCore.@addpkg","page":"Packages","title":"DataToolkitCore.@addpkg","text":"@addpkg name::Symbol uuid::String\n\nRegister the package identified by name with UUID uuid. This package may now be used with @require $name.\n\nAll @addpkg statements should lie within a module's __init__ function.\n\nExample\n\n@addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n\n\n\n\n\n","category":"macro"},{"location":"packages/#Using-extra-packages","page":"Packages","title":"Using extra packages","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"@require","category":"page"},{"location":"packages/#DataToolkitCore.@require","page":"Packages","title":"DataToolkitCore.@require","text":"@require Package\n@require Package = \"UUID\"\n\n\n\n\n\n","category":"macro"},{"location":"packages/#Example","page":"Packages","title":"Example","text":"","category":"section"},{"location":"packages/","page":"Packages","title":"Packages","text":"module DataToolkitExample\n\nusing DataToolkitCore\nusing DataFrame\n\nfunction __init__()\n    @addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n    @addpkg DelimitedFiles \"8bb1440f-4735-579b-a4ab-409b98df4dab\"\nend\n\nfunction load(::DataLoader{:csv}, from::IOStream, ::Type{DataFrame})\n    @require CSV\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nfunction load(::DataLoader{:delimcsv}, from::IOStream, ::Type{DataFrame})\n    @require DelimitedFiles\n    result = DelimitedFiles.readdlm(from, ',', DataFrame)\n    close(from)\n    result\nend\n\nend","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"Packages that implement loaders with other packages are recommended to use Julia 1.9's Package Extensions, together with the @requires macro and invokelatest like so:","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"# CsvLoaderPkg/src/loader.jl\nfunction load(::DataLoader{:csv}, from::IOStream, t::Type{DataFrame})\n    @require CSV\n    invokelatest(_load_csv, from, t)\nend","category":"page"},{"location":"packages/","page":"Packages","title":"Packages","text":"# CsvLoaderPkg/ext/csv.jl\nmodule csv\n\nusing CSV\nimport CsvLoaderPkg: _load_csv\n\nfunction _load_csv(from::IOStream, ::Type{DataFrame})\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nend","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"EditURL=\"index.org\"","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#The-problem-with-the-current-state-of-affairs","page":"Introduction","title":"The problem with the current state of affairs","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Data is beguiling. It can initially seem simple to deal with: \"here I have a file, and that's it\". However as soon as you do things with the data you're prone to be asked tricky questions like:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"where's the data?\nhow did you process that data?\nhow can I be sure I'm looking at the same data as you?","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This is no small part of the replication crisis.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Further concerns arise as soon as you start dealing with large quantities of data, or computationally expensive derived data sets. For example:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Have I already computed this data set somewhere else?\nIs my generated data up to date with its sources/dependencies?","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Generic tools exist for many parts of this problem, but there are some benefits that can be realised by creating a Julia-specific system, namely:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Having all pertinent environmental information in the data processing contained in a single Project.toml\nImproved convenience in data loading and management, compared to a generic solution\nAllowing datasets to be easily shared with a Julia package","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In addition, the Julia community seems to have a strong tendency to NIH[NIH] tools, so we may as well get ahead of this and try to make something good üòõ.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[NIH]: Not Invented Here, a tendency to \"reinvent the wheel\" to avoid using tools from external origins ‚Äî it would of course be better if you (re)made it.","category":"page"},{"location":"#Pre-existing-solutions","page":"Introduction","title":"Pre-existing solutions","text":"","category":"section"},{"location":"#DataLad","page":"Introduction","title":"DataLad","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Does a lot of things well\nPuts information on how to create data in git commit messages (bad)\nNo data file specification","category":"page"},{"location":"#Kedro-data-catalog","page":"Introduction","title":"Kedro data catalog","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Has a file defining all the data (good)\nHas poor versioning\nhttps://kedro.readthedocs.io/en/stable/data/data_catalog.html\nData Catalog CLI","category":"page"},{"location":"#Snakemake","page":"Introduction","title":"Snakemake","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Workflow manager, with remote file support\nSnakemake Remote Files\nGood list of possible file locations to handle\nDrawback is that you have to specify the location you expect(S3, http, FTP, etc.)\nNo data file specification","category":"page"},{"location":"#Nextflow","page":"Introduction","title":"Nextflow","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Workflow manager, with remote file support\nDocs on files and IO\nDocs on S3\nYou just call file() and nextflow figures out under the hood the protocol whether it should pull it from S3, http, FTP, or a local file.\nNo data file specification","category":"page"}]
}
