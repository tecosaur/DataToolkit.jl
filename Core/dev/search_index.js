var documenterSearchIndex = {"docs":
[{"location":"linting/","page":"Linting","title":"Linting","text":"EditURL=\"linting.org\"","category":"page"},{"location":"linting/#Linting","page":"Linting","title":"Linting","text":"","category":"section"},{"location":"linting/","page":"Linting","title":"Linting","text":"In DataToolkit, the Data.toml can be linted to identify and resolve potential issues. This is done by generating LintReport​s and LintItem​s.","category":"page"},{"location":"linting/","page":"Linting","title":"Linting","text":"LintReport\nLintItem","category":"page"},{"location":"linting/#DataToolkitCore.LintReport","page":"Linting","title":"DataToolkitCore.LintReport","text":"LintReport\n\nA collection of LintItems that apply to a particular DataCollection.\n\nDepending on the constructor called, the report may be for the entire collection or just a single DataSet.\n\nConstructors\n\nLintReport(collection::DataCollection) -> LintReport\nLintReport(dataset::DataSet) -> LintReport\n\n\n\n\n\n","category":"type"},{"location":"linting/#DataToolkitCore.LintItem","page":"Linting","title":"DataToolkitCore.LintItem","text":"Representation of a lint item.\n\nConstructors\n\nLintItem(source, severity::Union{Int, Symbol}, id::Symbol, message::String,\n         fixer::Union{Function, Nothing}=nothing, autoapply::Bool=false)\n\nsource is the object that the lint applies to.\n\nseverity should be one of the following values:\n\n0 or :debug, for messages that may assist with debugging problems that may be associated with particular configuration.\n1 or :info, for informational messages understandable to end-users.\n2 or :warning, for potentially harmful situations.\n3 or :error, for severe issues that will prevent normal functioning.\n\nid is a symbol representing the type of lint (e.g. :unknown_driver)\n\nmessage is a message, intelligible to the end-user, describing the particular nature of the issue with respect to source. It should be as specific as possible.\n\nfixer can be set to a function which modifies source to resolve the issue. If autoapply is set to true then fixer will be called spontaneously. The function should return true or false to indicate whether it was able to successfully fix the issue.\n\nAs a general rule, fixers that do or might require user input should not be run automatically, and fixers that can run without any user input and always \"do the right thing\" should be run automatically.\n\nExamples\n\nTODO\n\nStructure\n\nstruct LintItem{S}\n    source    ::S\n    severity  ::UInt8\n    id        ::Symbol\n    message   ::String\n    fixer     ::Union{Function, Nothing}\n    autoapply ::Bool\nend\n\n\n\n\n\n","category":"type"},{"location":"linting/","page":"Linting","title":"Linting","text":"Additional lint rules can be created by implementing new lint methods.","category":"page"},{"location":"linting/","page":"Linting","title":"Linting","text":"lint","category":"page"},{"location":"linting/#DataToolkitCore.lint","page":"Linting","title":"DataToolkitCore.lint","text":"lint(obj::T)\n\nCall all of the relevant linter functions on obj. More specifically, the method table is searched for lint(obj::T, ::Val{:linter_id}) methods (where :linter_id is a stand-in for the actual IDs used), and each specific lint function is invoked and the results combined.\n\nnote: Note\nEach specific linter function should return a vector of relevant LintItems, i.e.lint(obj::T, ::Val{:linter_id}) -> Union{Vector{LintItem{T}}, LintItem{T}, Nothing}See the documentation on LintItem for more information on how it should be constructed.\n\n\n\n\n\n","category":"function"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"EditURL=\"plugins.org\"","category":"page"},{"location":"plugins/#Plugins-and-Advice","page":"Plugins & Advice","title":"Plugins & Advice","text":"","category":"section"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"In DataToolkit, the plugin system enables key behaviour to be completely transformed when operating on a given DataCollection.","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"Plugin\n@dataplugin","category":"page"},{"location":"plugins/#DataToolkitCore.Plugin","page":"Plugins & Advice","title":"DataToolkitCore.Plugin","text":"Plugin\n\nA named collection of Advice that accompanies DataCollections.\n\nThe complete collection of advice provided by all plugins of a DataCollection is applied to every @advised call involving the DataCollection.\n\nSee also: Advice, AdviceAmalgamation.\n\nConstruction\n\nPlugin(name::String, advisors::Vector{Advice}) -> Plugin\nPlugin(name::String, advisors::Vector{<:Function}) -> Plugin\n\n\n\n\n\n","category":"type"},{"location":"plugins/#DataToolkitCore.@dataplugin","page":"Plugins & Advice","title":"DataToolkitCore.@dataplugin","text":"@dataplugin plugin_variable\n@dataplugin plugin_variable :default\n\nRegister the plugin given by the variable plugin_variable, along with its documentation (fetched by @doc). Should :default be given as the second argument the plugin is also added to the list of default plugins.\n\nThis effectively serves as a minor, but appreciable, convenience for the following pattern:\n\npush!(PLUGINS, myplugin)\nPLUGINS_DOCUMENTATION[myplugin.name] = @doc myplugin\npush!(DEFAULT_PLUGINS, myplugin.name) # when also adding to defaults\n\n\n\n\n\n","category":"macro"},{"location":"plugins/#Advice","page":"Plugins & Advice","title":"Advice","text":"","category":"section"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"Inspired by Lisp, DataToolkitCore comes with a method of completely transforming its behaviour at certain defined points. This is essentially a restricted form of Aspect-oriented programming. At certain declared locations (termed \"join points\"), we consult a list of \"advise\" functions that modify the execution at that point, and apply the (matched via \"pointcuts\") advise functions accordingly.","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"(Image: image)","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"Each applied advise function is wrapped around the invocation of the join point, and is able to modify the arguments, execution, and results of the join point.","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"(Image: image)","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"Advice\nAdviceAmalgamation\n@advise","category":"page"},{"location":"plugins/#DataToolkitCore.Advice","page":"Plugins & Advice","title":"DataToolkitCore.Advice","text":"Advice{func, context} <: Function\n\nAdvices allow for composable, highly flexible modifications of data by encapsulating a function call. They are inspired by elisp's advice system, namely the most versatile form — :around advice, and Clojure's transducers.\n\nA Advice is essentially a function wrapper, with a priority::Int attribute. The wrapped functions should be of the form:\n\n(action::Function, args...; kargs...) ->\n    ([post::Function], action::Function, args::Tuple, [kargs::NamedTuple])\n\nShort-hand return values with post or kargs omitted are also accepted, in which case default values (the identity function and (;) respectively) will be automatically substituted in.\n\n    input=(action args kwargs)\n         ┃                 ┏╸post=identity\n       ╭─╂────advisor 1────╂─╮\n       ╰─╂─────────────────╂─╯\n       ╭─╂────advisor 2────╂─╮\n       ╰─╂─────────────────╂─╯\n       ╭─╂────advisor 3────╂─╮\n       ╰─╂─────────────────╂─╯\n         ┃                 ┃\n         ▼                 ▽\naction(args; kargs) ━━━━▶ post╺━━▶ result\n\nTo specify which transforms a Advice should be applied to, ensure you add the relevant type parameters to your transducing function. In cases where the transducing function is not applicable, the Advice will simply act as the identity function.\n\nAfter all applicable Advices have been applied, action(args...; kargs...) |> post is called to produce the final result.\n\nThe final post function is created by rightwards-composition with every post entry of the advice forms (i.e. at each stage post = post ∘ extra is run).\n\nThe overall behaviour can be thought of as shells of advice.\n\n        ╭╌ advisor 1 ╌╌╌╌╌╌╌╌─╮\n        ┆ ╭╌ advisor 2 ╌╌╌╌╌╮ ┆\n        ┆ ┆                 ┆ ┆\ninput ━━┿━┿━━━▶ function ━━━┿━┿━━▶ result\n        ┆ ╰╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╯ ┆\n        ╰╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╯\n\nConstructors\n\nAdvice(priority::Int, f::Function)\nAdvice(f::Function) # priority is set to 1\n\nExamples\n\n1. Logging every time a DataSet is loaded.\n\nloggingadvisor = Advice(\n    function(post::Function, f::typeof(load), loader::DataLoader, input, outtype)\n        @info \"Loading $(loader.data.name)\"\n        (post, f, (loader, input, outtype))\n    end)\n\n2. Automatically committing each data file write.\n\nwritecommitadvisor = Advice(\n    function(post::Function, f::typeof(write), writer::DataWriter{:filesystem}, output, info)\n        function writecommit(result)\n            run(`git add $output`)\n            run(`git commit -m \"update $output\"`)\n            result\n        end\n        (post ∘ writecommit, writefn, (output, info))\n    end)\n\n\n\n\n\n","category":"type"},{"location":"plugins/#DataToolkitCore.AdviceAmalgamation","page":"Plugins & Advice","title":"DataToolkitCore.AdviceAmalgamation","text":"AdviceAmalgamation\n\nAn AdviceAmalgamation is a collection of Advices sourced from available Plugins.\n\nLike individual Advices, an AdviceAmalgamation can be called as a function. However, it also supports the following convenience syntax:\n\n(::AdviceAmalgamation)(f::Function, args...; kargs...) # -> result\n\nConstructors\n\nAdviceAmalgamation(advisors::Vector{Advice}, plugins_wanted::Vector{String}, plugins_used::Vector{String})\nAdviceAmalgamation(plugins::Vector{String})\nAdviceAmalgamation(collection::DataCollection)\n\n\n\n\n\n","category":"type"},{"location":"plugins/#DataToolkitCore.@advise","page":"Plugins & Advice","title":"DataToolkitCore.@advise","text":"@advise [source] f(args...; kwargs...)\n\nConvert a function call f(args...; kwargs...) to an advised function call, where the advise collection is obtained from source or the first data-like* value of args.\n\n* i.e. a DataCollection, DataSet, or DataTransformer\n\nFor example, @advise myfunc(other, somedataset, rest...) is equivalent to somedataset.collection.advise(myfunc, other, somedataset, rest...).\n\nThis macro performs a fairly minor code transformation, but should improve clarity.\n\nConsider adding a typeassert where type stability is important.\n\n\n\n\n\n","category":"macro"},{"location":"plugins/#Advisement-(join)-points","page":"Plugins & Advice","title":"Advisement (join) points","text":"","category":"section"},{"location":"plugins/#Parsing-and-serialisation-of-data-sets-and-collections","page":"Plugins & Advice","title":"Parsing and serialisation of data sets and collections","text":"","category":"section"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"DataCollection​s, DataSet​s, and DataTransformer​s are advised at two stages during parsing:","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"When calling fromspec on the Dict representation, at the start of parsing\nAt the end of the fromspec function, calling identity on the object","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"Serialisation is performed through the tospec call, which is also advised.","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"The signatures of the advised function calls are as follows:","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"fromspec(DataCollection, spec::Dict{String, Any}; path::Union{String, Nothing})::DataCollection\nidentity(collection::DataCollection)::DataCollection\ntospec(collection::DataCollection)::Dict","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"fromspec(DataSet, collection::DataCollection, name::String, spec::Dict{String, Any})::DataSet\nidentity(dataset::DataSet)::DataSet\ntospec(dataset::DataSet)::Dict","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"fromspec(DT::Type{<:DataTransformer}, dataset::DataSet, spec::Dict{String, Any})::DT\nidentity(dt::DataTransformer)::DataTransformer\ntospec(dt::DataTransformer)::Dict","category":"page"},{"location":"plugins/#Processing-identifiers","page":"Plugins & Advice","title":"Processing identifiers","text":"","category":"section"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"Both the parsing of an Identifier from a string, and the serialisation of an Identifier to a string are advised. Specifically, the following function calls:","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"parse_ident(spec::AbstractString)\nstring(ident::Identifier)","category":"page"},{"location":"plugins/#The-data-flow-arrows","page":"Plugins & Advice","title":"The data flow arrows","text":"","category":"section"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"The reading, writing, and storage of data may all be advised. Specifically, the following function calls:","category":"page"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"load(loader::DataLoader, datahandle, as::Type)\nstorage(provider::DataStorage, as::Type; write::Bool)\nsave(writer::DataWriter, datahandle, info)","category":"page"},{"location":"plugins/#Index-of-advised-calls-(join-points)","page":"Plugins & Advice","title":"Index of advised calls (join points)","text":"","category":"section"},{"location":"plugins/","page":"Plugins & Advice","title":"Plugins & Advice","text":"using Markdown\ncontent = Any[]\n\nconst AdviseRecord = NamedTuple{(:location, :parent, :invocation), Tuple{LineNumberNode, <:Union{Expr, Symbol}, Expr}}\nfunction findadvice!(acc::Vector{AdviseRecord}, expr::Expr; parent=nothing)\n    if expr.head == :macrocall && first(expr.args) == Symbol(\"@advise\")\n        !isnothing(parent) || @warn \"Macro @$(expr.args[2]) has no parent function\"\n        invocation = expr.args[end]\n        if Meta.isexpr(invocation, :(::), 2)\n            invocation = invocation.args[1]\n        end\n        push!(acc, (; location=expr.args[2], parent, invocation))\n    else\n        if isnothing(parent) && expr.head == :function\n            parent = if first(expr.args) isa Expr\n                first(first(expr.args).args)\n            else\n                first(expr.args)\n            end\n        elseif isnothing(parent) && expr.head == :(=) &&\n            first(expr.args) isa Expr && first(expr.args).head == :call\n            parent = first(first(expr.args).args)\n        end\n        findadvice!.(Ref(acc), expr.args; parent)\n    end\nend\nfindadvice!(acc, ::Any; parent=nothing) = nothing\n\nalladvice = Vector{AdviseRecord}()\nfor (root, dirs, files) in walkdir(\"../../src\")\n    for file in files\n        file == \"precompile.jl\" && continue\n        @info \"Analysing $file for advise\"\n        path = joinpath(root, file)\n        expr = Meta.parseall(read(path, String); filename=path)\n        findadvice!(alladvice, expr)\n    end\nend\n\nAdvItem = NamedTuple{(:line, :parent, :invocation), Tuple{Int, Union{Expr, Symbol}, Expr}}\nadvbyfunc = Dict{Symbol, Dict{Symbol, Vector{AdvItem}}}()\natypes = first.(getfield.(getfield.(alladvice, :invocation), :args)) |> unique\nafiles = getfield.(getfield.(alladvice, :location), :file) |> unique\n\nfor atype in atypes\n    advs = filter(a -> first(a.invocation.args) == atype, alladvice)\n    advbyfunc[atype] = Dict{Symbol, Vector{AdvItem}}()\n    for (; location, parent, invocation) in advs\n        if !haskey(advbyfunc[atype], location.file)\n            advbyfunc[atype][location.file] = Vector{AdvItem}()\n        end\n        push!(advbyfunc[atype][location.file], (; line=location.line, parent, invocation))\n    end\nend\n\npush!(content, Markdown.Paragraph([\n    \"There are \", Markdown.Bold(string(length(alladvice))),\n    \" advised function calls, across \",\n    Markdown.Bold(string(length(unique(getfield.(getfield.(alladvice, :location), :file))))),\n    \" files, covering \", Markdown.Bold(string(length(advbyfunc))),\n    \" functions (automatically detected).\"]))\n\npush!(content, Markdown.Header{3}([\"Arranged by function\"]))\n\nfor fname in sort(keys(advbyfunc) |> collect)\n    instances = advbyfunc[fname]\n    nadv = sum(length, values(instances))\n    push!(content, Markdown.Header{4}([\n        Markdown.Code(String(fname)),\n        if nadv == 1\n            \" (1 instance)\"\n        else\n            \" ($nadv instances)\"\n        end]))\n    list = Markdown.List(Any[], -1, false)\n    for file in sort(keys(instances) |> collect)\n        details = instances[file]\n        sublist = Markdown.List(Any[], -1, false)\n        for (; line, parent, invocation) in details\n            push!(sublist.items, Markdown.Paragraph(\n                [\"On line \", string(line), \" \",\n                 Markdown.Code(string(invocation)),\n                 \" is advised within a \",\n                 Markdown.Code(string(parent)), \" method.\"]))\n        end\n        push!(list.items, Any[\n            Markdown.Paragraph([Markdown.Italic(last(splitpath(String(file))))]),\n            sublist])\n    end\n    push!(content, list)\nend\n\npush!(content, Markdown.Header{3}([\"Arranged by file\"]))\n\nadvbyfile = Dict{Symbol, Vector{AdvItem}}()\nfor (; location, parent, invocation) in alladvice\n    if !haskey(advbyfile, location.file)\n        advbyfile[location.file] = Vector{AdvItem}()\n    end\n    push!(advbyfile[location.file], (; line=location.line, parent, invocation))\nend\n\nfor file in sort(afiles)\n    instances = advbyfile[file]\n    push!(content, Markdown.Header{5}([\n        Markdown.Code(last(splitpath(String(file)))),\n        if length(instances) == 1\n            \" (1 instance)\"\n        else\n            \" ($(length(instances)) instances)\"\n        end]))\n    list = Markdown.List(Any[], -1, false)\n    for (; line, parent, invocation) in instances\n        push!(list.items, [Markdown.Paragraph(\n            [\"On line \", string(line), \" \",\n             Markdown.Code(string(invocation)),\n             \" is advised within a \",\n             Markdown.Code(string(parent)), \" method.\"])])\n    end\n    push!(content, list)\nend\n\nMarkdown.MD(content) |> string |> Markdown.parse","category":"page"},{"location":"errors/","page":"Errors","title":"Errors","text":"EditURL=\"errors.org\"","category":"page"},{"location":"errors/#Errors","page":"Errors","title":"Errors","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"This package tries to minimise the use of generic errors, and maximise the helpfulness of error messages. To that end, a number of new error types are defined.","category":"page"},{"location":"errors/#Identifier-exceptions","page":"Errors","title":"Identifier exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnresolveableIdentifier\nAmbiguousIdentifier","category":"page"},{"location":"errors/#DataToolkitCore.UnresolveableIdentifier","page":"Errors","title":"DataToolkitCore.UnresolveableIdentifier","text":"UnresolveableIdentifier{T}(identifier::Union{String, UUID}, [collection::DataCollection]) <: IdentifierException\n\nNo T (optionally from collection) could be found that matches identifier.\n\nExample occurrences\n\njulia> d\"iirs\"\nERROR: UnresolveableIdentifier: \"iirs\" does not match any available data sets\n  Did you perhaps mean to refer to one of these data sets?\n    ■:iris (75% match)\nStacktrace: [...]\n\njulia> d\"iris::Int\"\nERROR: UnresolveableIdentifier: \"iris::Int\" does not match any available data sets\n  Without the type restriction, however, the following data sets match:\n    dataset:iris, which is available as a DataFrame, Matrix, CSV.File\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.AmbiguousIdentifier","page":"Errors","title":"DataToolkitCore.AmbiguousIdentifier","text":"AmbiguousIdentifier(identifier::Union{String, UUID}, matches::Vector, [collection]) <: IdentifierException\n\nSearching for identifier (optionally within collection), found multiple matches (provided as matches).\n\nExample occurrence\n\njulia> d\"multimatch\"\nERROR: AmbiguousIdentifier: \"multimatch\" matches multiple data sets\n    ■:multimatch [45685f5f-e6ff-4418-aaf6-084b847236a8]\n    ■:multimatch [92be4bda-55e9-4317-aff4-8d52ee6a5f2c]\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#Package-exceptions","page":"Errors","title":"Package exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"UnregisteredPackage\nMissingPackage","category":"page"},{"location":"errors/#DataToolkitCore.UnregisteredPackage","page":"Errors","title":"DataToolkitCore.UnregisteredPackage","text":"UnregisteredPackage(pkg::Symbol, mod::Module) <: PackageException\n\nThe package pkg was asked for within mod, but has not been registered by mod, and so cannot be loaded.\n\nExample occurrence\n\njulia> @require Foo\nERROR: UnregisteredPackage: Foo has not been registered by Main, see @addpkg for more information\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.MissingPackage","page":"Errors","title":"DataToolkitCore.MissingPackage","text":"MissingPackage(pkg::Base.PkgId) <: PackageException\n\nThe package pkg was asked for, but does not seem to be available in the current environment.\n\nExample occurrence\n\njulia> @addpkg Bar \"00000000-0000-0000-0000-000000000000\"\nBar [00000000-0000-0000-0000-000000000000]\n\njulia> @require Bar\n[ Info: Lazy-loading Bar [00000000-0000-0000-0000-000000000001]\nERROR: MissingPackage: Bar [00000000-0000-0000-0000-000000000001] has been required, but does not seem to be installed.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#Data-Operation-exceptions","page":"Errors","title":"Data Operation exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"CollectionVersionMismatch\nEmptyStackError\nReadonlyCollection\nTransformerError\nUnsatisfyableTransformer\nOrphanDataSet\nInvalidParameterType","category":"page"},{"location":"errors/#DataToolkitCore.CollectionVersionMismatch","page":"Errors","title":"DataToolkitCore.CollectionVersionMismatch","text":"CollectionVersionMismatch(version::Int) <: DataOperationException\n\nThe version of the collection currently being acted on is not supported by the current version of DataToolkitCore.\n\nExample occurrence\n\njulia> fromspec(DataCollection, Dict{String, Any}(\"data_config_version\" => -1))\nERROR: CollectionVersionMismatch: -1 (specified) ≠ 0 (current)\n  The data collection specification uses the v-1 data collection format, however\n  the installed DataToolkitCore version expects the v0 version of the format.\n  In the future, conversion facilities may be implemented, for now though you\n  will need to manually upgrade the file to the v0 format.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.EmptyStackError","page":"Errors","title":"DataToolkitCore.EmptyStackError","text":"EmptyStackError() <: DataOperationException\n\nAn attempt was made to perform an operation on a collection within the data stack, but the data stack is empty.\n\nExample occurrence\n\njulia> getlayer() # with an empty STACK\nERROR: EmptyStackError: The data collection stack is empty\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.ReadonlyCollection","page":"Errors","title":"DataToolkitCore.ReadonlyCollection","text":"ReadonlyCollection(collection::DataCollection) <: DataOperationException\n\nModification of collection is not viable, as it is read-only.\n\nExample Occurrence\n\njulia> lockedcollection = DataCollection(Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128)), \"config\" => Dict{String, Any}(\"locked\" => true)))\njulia> write(lockedcollection)\nERROR: ReadonlyCollection: The data collection unnamed#298 is locked\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.TransformerError","page":"Errors","title":"DataToolkitCore.TransformerError","text":"TransformerError(msg::String) <: DataOperationException\n\nA catch-all for issues involving data transformers, with details given in msg.\n\nExample occurrence\n\njulia> emptydata = DataSet(DataCollection(), \"empty\", Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata)\nERROR: TransformerError: Data set \"empty\" could not be loaded in any form.\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.UnsatisfyableTransformer","page":"Errors","title":"DataToolkitCore.UnsatisfyableTransformer","text":"UnsatisfyableTransformer{T}(dataset::DataSet, types::Vector{QualifiedType}) <: DataOperationException\n\nA transformer (of type T) that could provide any of types was asked for, but there is no transformer that satisfies this restriction.\n\nExample occurrence\n\njulia> emptydata = DataSet(DataCollection(), \"empty\", Dict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata, String)\nERROR: UnsatisfyableTransformer: There are no loaders for \"empty\" that can provide a String. The defined loaders are as follows:\nStacktrace: [...]\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.OrphanDataSet","page":"Errors","title":"DataToolkitCore.OrphanDataSet","text":"OrphanDataSet(dataset::DataSet) <: DataOperationException\n\nThe data set (dataset) is no longer a child of its parent collection.\n\nThis error should not occur, and is intended as a sanity check should something go quite wrong.\n\n\n\n\n\n","category":"type"},{"location":"errors/#DataToolkitCore.InvalidParameterType","page":"Errors","title":"DataToolkitCore.InvalidParameterType","text":"InvalidParameterType{T}(thing::T, parameter::String, type::Type) <: DataOperationException\n\nThe parameter parameter of thing must be of type type, but is not.\n\n\n\n\n\n","category":"type"},{"location":"errors/#Qualified-type-exceptions","page":"Errors","title":"Qualified type exceptions","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"ImpossibleTypeException","category":"page"},{"location":"errors/#DataToolkitCore.ImpossibleTypeException","page":"Errors","title":"DataToolkitCore.ImpossibleTypeException","text":"ImpossibleTypeException(qt::QualifiedType, mod::Union{Module, Nothing}) <: DataOperationException\n\nThe qualified type qt could not be converted to a Type, for some reason or another (mod is the parent module used in the attempt, should it be successfully identified, and nothing otherwise).\n\n\n\n\n\n","category":"type"},{"location":"errors/#Log-exception","page":"Errors","title":"Log exception","text":"","category":"section"},{"location":"errors/","page":"Errors","title":"Errors","text":"DataToolkitCore.LogTaskError","category":"page"},{"location":"errors/#DataToolkitCore.LogTaskError","page":"Errors","title":"DataToolkitCore.LogTaskError","text":"LogTaskError <: Exception\n\nA thin wrapper around a TaskFailedException that only prints the stack trace of the exception within the task.\n\n\n\n\n\n","category":"type"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"EditURL=\"transformers.org\"","category":"page"},{"location":"transformers/#Transformers","page":"Transformers","title":"Transformers","text":"","category":"section"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"The way DataToolkit actually interacts with datasets in through transformers.","category":"page"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"(Image: image)","category":"page"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"details: Applying a category theory lens\nIf we consider storage locations (storage), raw data (data), and informative representations of the data (information) to be categories, then if you squint a bit the storage, loader, and writer transformers can be viewed as functors. This doesn't make much of a practical impact, but I think it's a fun way of framing things. (Image: image)","category":"page"},{"location":"transformers/#Transformer-types","page":"Transformers","title":"Transformer types","text":"","category":"section"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"DataTransformer\nDataStorage\nDataLoader\nDataWriter","category":"page"},{"location":"transformers/#DataToolkitCore.DataTransformer","page":"Transformers","title":"DataToolkitCore.DataTransformer","text":"DataTransformer{kind, driver}\n\nThe parent type for structures producing or consuming data.\n\n                 ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n                 ▲               ╷\n                 ╰────writer─────╯\n\nThere are three kinds of specialised DataTransformers:\n\nDataStorage\nDataLoader\nDataWriter\n\nEach transformer takes a Symbol type parameter designating the driver which should be used to perform the data operation.\n\nIn addition, each transformer has the following fields:\n\ndataset::DataSet, the data set the method operates on\ntype::Vector{QualifiedType}, the Julia types the method supports\npriority::Int, the priority with which this method should be used, compared to alternatives. Lower values have higher priority.\nparameters::Dict{String, Any}, any parameters applied to the method.\n\nSee also: DataStorage, DataLoader, DataWriter, supportedtypes.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#DataToolkitCore.DataStorage","page":"Transformers","title":"DataToolkitCore.DataStorage","text":"DataStorage <: DataTransformer\n\nA DataTransformer that can retrieve data from a source, and/or store data in a source.\n\n  Storage ◀────▶ Data\n\nTypically a DataStorage will have methods implemented to provide storage as a FilePath or IO, and potentially writable IO or a FilePath that can be written to.\n\nData of a certain form retrieved from a storage backend of a DataSet can be accessed by calling open on the dataset.\n\nSee also: storage, getstorage, putstorage.\n\nImplementing a DataStorage backend\n\nThere are two ways a new DataStorage backend can be implemented:\n\nImplementing a single storage(ds::DataStorage{:name}, as::Type; write::Bool) method, that will provide an as handle for ds, in either read or write mode.\nImplement one or both of the following methods:\ngetstorage(ds::DataStorage{:name}, as::Type)\nputstorage(ds::DataStorage{:name}, as::Type)\n\nThis split approach allows for backends with very similar read/write cases to be easily implemented with a single storage method, while also allowing for more backends with very different read/write methods or that only support reading or writing exclusively to only implement the relevant method.\n\nOptionally, the following extra methods can be implemented:\n\nsupportedtypes when storage can be read/written to multiple forms, to give preference to certain types and help DataToolkit make reasonable assumptions (does nothing when only a single concrete type is supported)\ncreateauto and/or createinteractive to improve the user experience when creating instances of the storage backend.\ncreatepriority, when you want to have automatic creation using this storage backend to be tried earlier or later than default by DataToolkit.\n\nExample storage backend implementation\n\nFor simple cases, it can only take a few lines to implement a storage backend.\n\nThis is the actual implementation of the :filesystem backend from DataToolkitCommon,\n\nfunction storage(storage::DataStorage{:filesystem}, ::Type{FilePath}; write::Bool)\n    path = getpath(storage)\n    if @advise storage isfile(path)\n        FilePath(path)\n    end\nend\n\nfunction storage(storage::DataStorage{:filesystem}, ::Type{DirPath}; write::Bool)\n    path = getpath(storage)\n    if @advise storage isdir(path)\n        DirPath(path)\n    end\nend\n\nThis provides support for both files and directories, assisted by the helper function getpath, which retrieves the \"path\" parameter using @getparam and then normalises it.\n\nThe isfile/isdir calls are wrapped in @advise to allow plugins to dynamically perform additional or even potentially instantiate a file on-demand.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#DataToolkitCore.DataLoader","page":"Transformers","title":"DataToolkitCore.DataLoader","text":"DataLoader <: DataTransformer\n\nA DataTransformer that interprets data into a useful form.\n\n    ╭────loader─────╮\n    ╵               ▼\n  Data          Information\n\nTypically a DataLoader will have methods implemented to interpret a raw data stream such as IO or a FilePath to a richer, more informative form (such as a DataFrame).\n\nA particular form can be loaded from a DataSet by calling read on the dataset.\n\nSee also: load, supportedtypes.\n\nImplementing a DataLoader backend\n\nTo provide a new DataLoader backend, you need to implement a load method that will provide the data in the requested form:\n\nload(::DataLoader{:name}, source, as::Type)\n\nOften the load implementation will make use of a helpful package. To avoid eagerly loading the package, you can make use of @require and the lazy loading system. In DataToolkitCommon this is combined with the package extension system, resulting in loader implementations that look something like this:\n\nfunction load(loader::DataLoader{:name}, from::IO, as::Vector{String})\n    @require SomePkg\n    param = @getparam loader.\"param\"::Int 0\n    invokelatest(_load_somepkg, from, param)\nend\n\nfunction _load_somepkg end # Implemented in a package extension\n\nDepending on the number of loaders and other details this may be overkill in some situations.\n\nIn order to matchmake DataLoaders and DataStorages, DataToolkit engages in what is essentially custom dispatch using reflection and method table interrogation. In order for this to work well, the source and as arguments should avoid using parametric types beyond the most simple case:\n\nload(::DataLoader{:name}, source::T, as::Type{T}) where {T}\n\nIn cases where a given DataLoader can provide multiple types, or Any/parametric types, you can hint which types are most preferred by implementing supportedtypes for the loader.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#DataToolkitCore.DataWriter","page":"Transformers","title":"DataToolkitCore.DataWriter","text":"DataWriter <: DataTransformer\n\nA DataTransformer that writes a representation of some information to a source.\n\n  Data          Information\n    ▲               ╷\n    ╰────writer─────╯\n\nTypically a DataWriter will have methods implemented to write a structured form of the information to a more basic data format such as IO or a FilePath.\n\nA compatible value can be written to a DataSet by calling write on the dataset.\n\nImplementing a DataWriter backend\n\nTo provide a new DataWriter backend, you need to implement a save method that can write a value to a certain form.\n\nsave(::DataWriter{:name}, destination, info)\n\nAs with DataLoaders, DataWriters can also make use of the lazy loading system and package extensions to avoid eager loading of packages.\n\nOften the save implementation will make use of a helpful package. To avoid eagerly saveing the package, you can make use of @require and the lazy saveing system. In DataToolkitCommon this is combined with the package extension system, resulting in saveer implementations that look something like this:\n\nfunction save(writer::DataWriter{:name}, dest::IO, info::Vector{String})\n    @require SomePkg\n    invokelatest(_save_somepkg, info)\nend\n\nfunction _save_somepkg end # Implemented in a package extension\n\nDepending on the number of loaders and other details this may be overkill in some situations.\n\nIn cases where a given DataWriter can provide multiple types, or Any/parametric types, you can hint which types are most preferred by implementing supportedtypes for the loader.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#Implementation-API","page":"Transformers","title":"Implementation API","text":"","category":"section"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"storage\ngetstorage\nputstorage\nload\nsave","category":"page"},{"location":"transformers/#DataToolkitCore.storage","page":"Transformers","title":"DataToolkitCore.storage","text":"storage(storer::DataStorage, as::Type; write::Bool=false)\n\nFetch the as from storer, appropiate for reading data from or writing data to (depending on write).\n\nBy default, this just calls getstorage or putstorage (depending on write).\n\nThis executes the following component of the overall data flow:\n\nStorage ◀────▶ Data\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.getstorage","page":"Transformers","title":"DataToolkitCore.getstorage","text":"getstorage(storer::DataStorage, as::Type)\n\nFetch the as form of storer, for reading data from.\n\nThis executes the following component of the overall data flow:\n\nStorage ─────▶ Data\n\nSee also: storage, putstorage.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.putstorage","page":"Transformers","title":"DataToolkitCore.putstorage","text":"putstorage(storer::DataStorage, as::Type)\n\nFetch a handle in the form as from storer, that data can be written to.\n\nThis executes the following component of the overall data flow:\n\nStorage ◀───── Data\n\nSee also: storage, getstorage.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.load","page":"Transformers","title":"DataToolkitCore.load","text":"load(loader::DataLoader{driver}, source::Any, as::Type)\n\nUsing a certain loader, obtain information in the form of as from the data given by source.\n\nThis fulfils this component of the overall data flow:\n\n  ╭────loader─────╮\n  ╵               ▼\nData          Information\n\nWhen the loader produces nothing this is taken to indicate that it was unable to load the data for some reason, and that another loader should be tried if possible. This can be considered a soft failure. Any other value is considered valid information.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.save","page":"Transformers","title":"DataToolkitCore.save","text":"save(writer::Datasaveer{driver}, destination::Any, information::Any)\n\nUsing a certain writer, save the information to the destination.\n\nThis fulfils this component of the overall data flow:\n\nData          Information\n  ▲               ╷\n  ╰────writer─────╯\n\n\n\n\n\n","category":"function"},{"location":"transformers/#Extras","page":"Transformers","title":"Extras","text":"","category":"section"},{"location":"transformers/","page":"Transformers","title":"Transformers","text":"supportedtypes\ncreateauto\ncreateinteractive\ncreatepriority","category":"page"},{"location":"transformers/#DataToolkitCore.supportedtypes","page":"Transformers","title":"DataToolkitCore.supportedtypes","text":"supportedtypes(DT::Type{<:DataTransformer}, [spec::Dict{String, Any}, dataset::DataSet]) -> Vector{QualifiedType}\n\nReturn a list of types supported by the data transformer DT.\n\nThis is used as the default value for the type key in the Data TOML. The list of types is dynamically generated based on the available methods for the data transformer.\n\nIn some cases, it makes sense for this to be explicitly defined for a particular transformer, optionally taking into account information in the spec and/or parent dataset.\n\nSee also: QualifiedType, DataTransformer.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.createauto","page":"Transformers","title":"DataToolkitCore.createauto","text":"createauto([dataset::DataSet], T::Type{<:DataTransformer}, source::String)\n\nAutomatically attempts to create a data transformer of type T using source and optionally dataset, without requiring user interaction. Returns either a specification for the transformer as a Dict{String, Any}, true to indicate that an empty (no parameters) transformer should be created, or false/nothing if the transformer cannot be created automatically.\n\nSpecific transformers should implement specialised forms of this function, either returning nothing if automatic creation is not possible, or a \"create spec form\" as a list of key::String => value pairs. For example:\n\n[\"foo\" => \"bar\",\n \"baz\" => 2]\n\nUse this function when the creation process should be handled programmatically without user input. If user interaction is required to gather additional information use createinteractive.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.createinteractive","page":"Transformers","title":"DataToolkitCore.createinteractive","text":"createinteractive([dataset::DataSet], T::Type{<:DataTransformer}, source::String)\n\nAttempts to create a data transformer of type T with user interaction, using source and dataset. Prompts the user for additional information if required. Returns either a specification for the transformer as a dictionary, true to indicate that an empty (no parameters) transformer should be created, or nothing if the transformer cannot be created interactively.\n\nSpecific transformers should implement specialised forms of this function, either returning nothing if creation is not applicable, or a \"create spec form\" as a list of key::String => value pairs. For example:\n\n[\"foo\" => \"bar\",  \"baz\" => 2]\n\nIn addition to accepting TOML-representable values, a NamedTuple can be used to define the interactive prompt with fields like:\n\n(; prompt::String = \"key\",    type::Type{String or Bool or <:Number} = String,    default::type = false or \"\",    optional::Bool = false,    skipvalue::Any = nothing,    post::Function = identity)\n\nThe function can also accept a Function that takes the current specification as an argument and returns a TOML-representable value or NamedTuple.\n\nUse this function when user interaction is necessary for the creation process. For cases where the creation can be handled programmatically without user input, consider using createauto.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#DataToolkitCore.createpriority","page":"Transformers","title":"DataToolkitCore.createpriority","text":"createpriority(T::Type{<:DataTransformer})\n\nThe priority with which a transformer of type T should be created. This can be any integer, but try to keep to -100–100 (see create).\n\n\n\n\n\n","category":"function"},{"location":"utilities/","page":"Utilities","title":"Utilities","text":"EditURL=\"utilities.org\"","category":"page"},{"location":"utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utilities/","page":"Utilities","title":"Utilities","text":"SystemPath\nFilePath\nDirPath\n@log_do\n@getparam\natomic_write","category":"page"},{"location":"utilities/#DataToolkitCore.SystemPath","page":"Utilities","title":"DataToolkitCore.SystemPath","text":"SystemPath\n\nA string, but one that explicitly refers to a path on the system.\n\nSee also: FilePath, DirPath.\n\n\n\n\n\n","category":"type"},{"location":"utilities/#DataToolkitCore.FilePath","page":"Utilities","title":"DataToolkitCore.FilePath","text":"FilePath <: SystemPath\n\nCrude stand in for a file path type, which is strangely absent from Base.\n\nThis allows for load/write method dispatch, and the distinguishing of file content (as a String) from file paths.\n\nSee also: DirPath.\n\nExamples\n\njulia> string(FilePath(\"some/path\"))\n\"some/path\"\n\n\n\n\n\n","category":"type"},{"location":"utilities/#DataToolkitCore.DirPath","page":"Utilities","title":"DataToolkitCore.DirPath","text":"DirPath <: SystemPath\n\nSignifies that a given string is in fact a path to a directory.\n\nThis allows for load/write method dispatch, and the distinguishing of file content (as a String) from file paths.\n\nSee also: FilePath.\n\nExamples\n\njulia> string(DirPath(\"some/path\"))\n\"some/path\"\n\n\n\n\n\n","category":"type"},{"location":"utilities/#DataToolkitCore.@log_do","page":"Utilities","title":"DataToolkitCore.@log_do","text":"@log_do category message [expr]\n\nReturn the result of expr, logging message with category if appropriate to do so.\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#DataToolkitCore.@getparam","page":"Utilities","title":"DataToolkitCore.@getparam","text":"@getparam container.\"parameter\"::Type default=nothing\n\nGet the parameter \"parameter\" from container (a DataCollection, DataSet, or DataTransformer), ensuring that it is of type Type. If it is not, an InvalidParameterType error is thrown.\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#DataToolkitCore.atomic_write","page":"Utilities","title":"DataToolkitCore.atomic_write","text":"atomic_write(f::Function, dest::AbstractString; temp::AbstractString = dest * \"_XXXX.part\")\n\nAtomically write to dest with f, via temp.\n\nCalls the function f that writes to temp, with temp given as an IO handle or a String depending on as. Upon completion, temp is renamed to dest.\n\nThe file dest is not touched until the write is complete, and if the write to dest is interrupted or fails for any reason, no data is written to temp.\n\nwarning: Limitations\nIt is impossible to gauntree truly atomic writes on hardware without power loss  protection (PLP), even with copy-on-write (CoW) filesystems. This function makes  a best effort, calling  fdatasync  before renaming a file. In most situations this will be sufficient, but it  is not a guarantee.\n\n\n\n\n\n","category":"function"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"EditURL=\"packages.org\"","category":"page"},{"location":"packages/#Lazily-using-packages","page":"Lazy Packages","title":"Lazily using packages","text":"","category":"section"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"It is entirely likely that in the course of writing a package providing a custom data transformer, one would come across packages that may be needed.","category":"page"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"Every possibly desired package could be shoved into the list of dependences, but this is a somewhat crude approach. A more granular approach is enabled with two macros, @addpkg and @require.","category":"page"},{"location":"packages/#Letting-DataToolkit-know-about-extra-packages","page":"Lazy Packages","title":"Letting DataToolkit know about extra packages","text":"","category":"section"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"addpkg\n@addpkg","category":"page"},{"location":"packages/#DataToolkitCore.addpkg","page":"Lazy Packages","title":"DataToolkitCore.addpkg","text":"addpkg(mod::Module, name::Symbol, uuid::Union{UUID, String})\n\nRegister the package identified by name with UUID uuid, as known by mod.\n\nSee also: @addpkg, @require.\n\n\n\n\n\n","category":"function"},{"location":"packages/#DataToolkitCore.@addpkg","page":"Lazy Packages","title":"DataToolkitCore.@addpkg","text":"@addpkg name::Symbol uuid::String\n\nRegister the package identified by name with UUID uuid. This package may now be used with @require $name.\n\nAll @addpkg statements should lie within a module's __init__ function.\n\nExample\n\n@addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n\nSee also: @require, addpkg.\n\n\n\n\n\n","category":"macro"},{"location":"packages/#Using-extra-packages","page":"Lazy Packages","title":"Using extra packages","text":"","category":"section"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"@require\ninvokepkglatest","category":"page"},{"location":"packages/#DataToolkitCore.@require","page":"Lazy Packages","title":"DataToolkitCore.@require","text":"@require Package\n@require Package = \"UUID\"\n\nRequire the package Package, either previously registered with @addpkg or by UUID.\n\nThis sets a variable Package to the module of the package.\n\nIf the package is not currently loaded, DataToolkit will attempt to lazy-load the package via an early return PkgRequiredRerunNeeded singleton. So long as this is seen by a calling invokepkglatest the package will be loaded and the function re-run.\n\nSee also: @addpkg, invokepkglatest.\n\n\n\n\n\n","category":"macro"},{"location":"packages/#DataToolkitCore.invokepkglatest","page":"Lazy Packages","title":"DataToolkitCore.invokepkglatest","text":"invokepkglatest(f, args...; kwargs...)\n\nCall f(args...; kwargs...) via invokelatest, and re-run if PkgRequiredRerunNeeded is returned.\n\nSee also: @require.\n\n\n\n\n\n","category":"function"},{"location":"packages/#Example","page":"Lazy Packages","title":"Example","text":"","category":"section"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"module DataToolkitExample\n\nusing DataToolkitCore\nusing DataFrame\n\nfunction __init__()\n    @addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n    @addpkg DelimitedFiles \"8bb1440f-4735-579b-a4ab-409b98df4dab\"\nend\n\nfunction load(::DataLoader{:csv}, from::IOStream, ::Type{DataFrame})\n    @require CSV\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nfunction load(::DataLoader{:delimcsv}, from::IOStream, ::Type{DataFrame})\n    @require DelimitedFiles\n    result = DelimitedFiles.readdlm(from, ',', DataFrame)\n    close(from)\n    result\nend\n\nend","category":"page"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"Packages that implement loaders with other packages are recommended to use Julia 1.9's Package Extensions, together with the @requires macro and invokelatest like so:","category":"page"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"# CsvLoaderPkg/src/loader.jl\nfunction load(::DataLoader{:csv}, from::IOStream, t::Type{DataFrame})\n    @require CSV\n    invokelatest(_load_csv, from, t)\nend","category":"page"},{"location":"packages/","page":"Lazy Packages","title":"Lazy Packages","text":"# CsvLoaderPkg/ext/csv.jl\nmodule csv\n\nusing CSV\nimport CsvLoaderPkg: _load_csv\n\nfunction _load_csv(from::IOStream, ::Type{DataFrame})\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nend","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"EditURL=\"index.org\"","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package implements the core design and API of DataToolkit. It is not intended to be user-facing and is instead for packages that want to build on the DataToolkit infrastructure itself.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This documentation also details some user-facing API reexported by other packages, how transformers and plugins work and should be implemented, as well as some general design decisions.","category":"page"},{"location":"#The-problem-with-the-current-state-of-affairs","page":"Introduction","title":"The problem with the current state of affairs","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Data is beguiling. It can initially seem simple to deal with: \"here I have a file, and that's it\". However as soon as you do things with the data you're prone to be asked tricky questions like:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"where's the data?\nhow did you process that data?\nhow can I be sure I'm looking at the same data as you?","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This is no small part of the replication crisis.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: image)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Further concerns arise as soon as you start dealing with large quantities of data, or computationally expensive derived data sets. For example:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Have I already computed this data set somewhere else?\nIs my generated data up to date with its sources/dependencies?","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Generic tools exist for many parts of this problem, but there are some benefits that can be realised by creating a Julia-specific system, namely:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Having all pertinent environmental information in the data processing contained in a single Project.toml\nImproved convenience in data loading and management, compared to a generic solution\nAllowing datasets to be easily shared with a Julia package","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In addition, the Julia community seems to have a strong tendency to NIH[NIH] tools, so we may as well get ahead of this and try to make something good 😛.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[NIH]: Not Invented Here, a tendency to \"reinvent the wheel\" to avoid using tools from external origins — it would of course be better if you (re)made it.","category":"page"},{"location":"#Pre-existing-solutions","page":"Introduction","title":"Pre-existing solutions","text":"","category":"section"},{"location":"#DataLad","page":"Introduction","title":"DataLad","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Does a lot of things well\nPuts information on how to create data in git commit messages (bad)\nNo data file specification","category":"page"},{"location":"#Kedro-data-catalog","page":"Introduction","title":"Kedro data catalog","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Has a file defining all the data (good)\nHas poor versioning\nhttps://kedro.readthedocs.io/en/stable/data/data_catalog.html\nData Catalog CLI","category":"page"},{"location":"#Snakemake","page":"Introduction","title":"Snakemake","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Workflow manager, with remote file support\nSnakemake Remote Files\nGood list of possible file locations to handle\nDrawback is that you have to specify the location you expect(S3, http, FTP, etc.)\nNo data file specification","category":"page"},{"location":"#Nextflow","page":"Introduction","title":"Nextflow","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Workflow manager, with remote file support\nDocs on files and IO\nDocs on S3\nYou just call file() and nextflow figures out under the hood the protocol whether it should pull it from S3, http, FTP, or a local file.\nNo data file specification","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"EditURL=\"internals.org\"","category":"page"},{"location":"internals/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#Type-transformations","page":"Internals","title":"Type transformations","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.issubtype\nDataToolkitCore.paramtypebound\nDataToolkitCore.targettypes\nDataToolkitCore.ispreferredpath\nDataToolkitCore.transformersigs\nDataToolkitCore.typesteps","category":"page"},{"location":"internals/#DataToolkitCore.issubtype","page":"Internals","title":"DataToolkitCore.issubtype","text":"issubtype(X::Type, T::Union{Type, TypeVar})\nissubtype(x::X, T::Union{Type, TypeVar})\n\nCheck if X is indeed a subtype of T.\n\nThis is a tweaked version of isa that can (mostly) handle TypeVar instances.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.paramtypebound","page":"Internals","title":"DataToolkitCore.paramtypebound","text":"paramtypebound(T::Union{Type, TypeVar}, Tparam::Union{Type, TypeVar}, paramT::Type)\n\nReturn the Type that bounds T.\n\nThis is simply T when T isa Type, but T may also be a TypeVar that is parameterised by Tparam. In this case, the Type that T is parameterised by is returned, which is taken to be paramT.\n\nGiven a type T that may be parameterised according to Tparam,\n\njulia> paramtypebound(String, IO, IO)\nString\n\njulia> T = TypeVar(:T)\nT\n\njulia> paramtypebound(T, Type{T}, Float64)\nFloat64\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.targettypes","page":"Internals","title":"DataToolkitCore.targettypes","text":"targettypes(types::Vector{QualifiedType}, desired::Type) -> Vector{Type}\ntargettypes(transformer::DataTransformer, desired::Type) -> Vector{Type}\n\nReturn all Types that one might hope to produce from types or transformer.\n\nMore specifically, this will give all Types that can be produced which are a subtype of desired, and desired itself.\n\nPriority order is preserved.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.ispreferredpath","page":"Internals","title":"DataToolkitCore.ispreferredpath","text":"ispreferredpath(a, b)\n\nCompares two \"type paths\" a and b, returning whether a is preferred.\n\nEach \"type path\" is a tuple of the form:\n\n(Tin::Type => Tout::Type, index::Int, transformer::Type{<:DataTransformer})\n\nThis operates on the following rules:\n\nThe path with the lower index is preferred.\nIf the indices are equal, the path with the more specific output type is preferred.\nIf the output types are equally specific, the path with the more specific loader is preferred.\nIf the loaders are equally specific, the more similar data transformation (Tin => Tout) is preferred.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.transformersigs","page":"Internals","title":"DataToolkitCore.transformersigs","text":"transformersigs(transformer::Type{<:DataTransformer}, desired::Type)\n\nReturn processed signatures of the transformation methods implemented for transformer that could produce/provide a subtype of desired.\n\nDataStorage produces tuples of (Type{<:DataStorage}, Type{out})\nDataLoaders produces tuples of (Type{<:DataLoader}, Type{in}, Type{out})\nDataWriter produces tuples of (Type{<:DataWriter}, Type{in}, Type{data})\n\nThe DataStorage method takes a write::Bool keyword argument.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.typesteps","page":"Internals","title":"DataToolkitCore.typesteps","text":"typesteps(loader::DataLoader, desired::Type) -> Vector{Pair{Type, Type}}\n\nIdentify and order all uses of loader that may produce a subtype of desired.\n\nMore specifically, this finds all load methods that can produce a subtype of desired, checks what input and output types they work with, and orders them according to the declared types of loader and the specificity of the output types (more specific is interpreted as better).\n\nThe output vector gives the step-change in the type domain that each method performs.\n\n\n\n\n\n","category":"function"},{"location":"internals/#TOML-related","page":"Internals","title":"TOML related","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.toml_safe\nDataToolkitCore.tomlreformat!\nDataToolkitCore.dataset_parameters","category":"page"},{"location":"internals/#DataToolkitCore.toml_safe","page":"Internals","title":"DataToolkitCore.toml_safe","text":"toml_safe(value)\n\nRecursively convert value to a form that DataToolkit can safely encode to TOML.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.tomlreformat!","page":"Internals","title":"DataToolkitCore.tomlreformat!","text":"tomlreformat!(io::IO)\n\nConsume io representing a TOML file, and reformat it to improve readability. Currently this takes the form of the following changes:\n\nReplace inline multi-line strings with multi-line toml strings.\n\nAn IOBuffer containing the reformatted content is returned.\n\nThe processing assumes that io contains TOML.print-formatted content. Should this not be the case, mangled TOML may be emitted.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.dataset_parameters","page":"Internals","title":"DataToolkitCore.dataset_parameters","text":"dataset_parameters(source::Union{DataCollection, DataSet, DataTransformer},\n                   action::Val{:extract|:resolve|:encode}, value::Any)\n\nObtain a form (depending on action) of value, a property within source.\n\nActions\n\n:extract  Look for DataSet references (\"📇DATASET<<…>>\") within   value, and turn them into Identifiers (the inverse of :encode).\n\n:resolve  Look for Identifiers in value, and resolve them to the   referenced DataSet/value.\n\n:encode  Look for Identifiers in value, and turn them into DataSet references   (the inverse of :extract).\n\n\n\n\n\n","category":"function"},{"location":"internals/#String-utils","page":"Internals","title":"String utils","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.natkeygen\nDataToolkitCore.stringdist\nDataToolkitCore.stringsimilarity\nDataToolkitCore.issubseq\nDataToolkitCore.longest_common_subsequence\nDataToolkitCore.highlight_lcs","category":"page"},{"location":"internals/#DataToolkitCore.natkeygen","page":"Internals","title":"DataToolkitCore.natkeygen","text":"natkeygen(key::String)\n\nGenerate a sorting key for key that when used with sort will put the collection in \"natural order\".\n\njulia> natkeygen.([\"A1\", \"A10\", \"A02\", \"A1.5\"])\n4-element Vector{Vector{String}}:\n [\"a\", \"0\\x01\"]\n [\"a\", \"0\\n\"]\n [\"a\", \"0\\x02\"]\n [\"a\", \"0\\x015\"]\n\njulia> sort([\"A1\", \"A10\", \"A02\", \"A1.5\"], by=natkeygen)\n4-element Vector{String}:\n \"A1\"\n \"A1.5\"\n \"A02\"\n \"A10\"\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.stringdist","page":"Internals","title":"DataToolkitCore.stringdist","text":"stringdist(a::AbstractString, b::AbstractString; halfcase::Bool=false)\n\nCalculate the Restricted Damerau-Levenshtein distance (aka. Optimal String Alignment) between a and b.\n\nThis is the minimum number of edits required to transform a to b, where each edit is a deletion, insertion, substitution, or transposition of a character, with the restriction that no substring is edited more than once.\n\nWhen halfcase is true, substitutions that just switch the case of a character cost half as much.\n\nExamples\n\njulia> stringdist(\"The quick brown fox jumps over the lazy dog\",\n                  \"The quack borwn fox leaps ovver the lzy dog\")\n7\n\njulia> stringdist(\"typo\", \"tpyo\")\n1\n\njulia> stringdist(\"frog\", \"cat\")\n4\n\njulia> stringdist(\"Thing\", \"thing\", halfcase=true)\n0.5\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.stringsimilarity","page":"Internals","title":"DataToolkitCore.stringsimilarity","text":"stringsimilarity(a::AbstractString, b::AbstractString; halfcase::Bool=false)\n\nReturn the stringdist as a proportion of the maximum length of a and b, take one. When halfcase is true, case switches cost half as much.\n\nExample\n\njulia> stringsimilarity(\"same\", \"same\")\n1.0\n\njulia> stringsimilarity(\"semi\", \"demi\")\n0.75\n\njulia> stringsimilarity(\"Same\", \"same\", halfcase=true)\n0.875\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.issubseq","page":"Internals","title":"DataToolkitCore.issubseq","text":"issubseq(a, b)\n\nReturn true if a is a subsequence of b, false otherwise.\n\nExamples\n\njulia> issubseq(\"abc\", \"abc\")\ntrue\n\njulia> issubseq(\"adg\", \"abcdefg\")\ntrue\n\njulia> issubseq(\"gda\", \"abcdefg\")\nfalse\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.longest_common_subsequence","page":"Internals","title":"DataToolkitCore.longest_common_subsequence","text":"longest_common_subsequence(a, b)\n\nFind the longest common subsequence of b within a, returning the indices of a that comprise the subsequence.\n\nThis function is intended for strings, but will work for any indexable objects with == equality defined for their elements.\n\nExample\n\njulia> longest_common_subsequence(\"same\", \"same\")\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\njulia> longest_common_subsequence(\"fooandbar\", \"foobar\")\n6-element Vector{Int64}:\n 1\n 2\n 3\n 7\n 8\n 9\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.highlight_lcs","page":"Internals","title":"DataToolkitCore.highlight_lcs","text":"highlight_lcs(io::IO, a::String, b::String;\n              before::String=\"\\e[1m\", after::String=\"\\e[22m\",\n              invert::Bool=false)\n\nPrint a, highlighting the longest common subsequence between a and b by inserting before prior to each subsequence region and after afterwards.\n\nIf invert is set, the before/after behaviour is switched.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Small-dicts","page":"Internals","title":"Small dicts","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.newdict\nDataToolkitCore.shrinkdict","category":"page"},{"location":"internals/#DataToolkitCore.newdict","page":"Internals","title":"DataToolkitCore.newdict","text":"newdict(K::Type, V::Type, capacity::Int) -> Dict{K, V}\n\nCreate a new Dict{K, V} sized to hold capacity elements, hopefully without resizing. Depending on the particular value of capacity and the Julia version, this can result in substantial memory savings for small dictionaries.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.shrinkdict","page":"Internals","title":"DataToolkitCore.shrinkdict","text":"shrinkdict(dict::Dict) -> Dict\n\nIf dict looks like it may be smaller if reconstructed using newdict, do so.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Logging","page":"Internals","title":"Logging","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.should_log\nDataToolkitCore.wait_maybe_log","category":"page"},{"location":"internals/#DataToolkitCore.should_log","page":"Internals","title":"DataToolkitCore.should_log","text":"should_log(category::String) -> Bool\n\nDetermine whether a message should be logged based on its category.\n\nThe category string can contain any number of subcategories separated by colons. If any parent category is enabled, the subcategory is also enabled.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.wait_maybe_log","page":"Internals","title":"DataToolkitCore.wait_maybe_log","text":"wait_maybe_log(category::String, message::AbstractString; mod::Module, file::String, line::Int) -> Timer\n\nWait for a delay before logging message with category if should_log(category).\n\nThe log is produced with metadata from mod, file, and line.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Miscilanious","page":"Internals","title":"Miscilanious","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.typeify\nDataToolkitCore.reinit\nDataToolkitCore._dataadvisecall\nDataToolkitCore.strip_stacktrace_advice!\nDataToolkitCore.get_package\nDataToolkitCore.try_install_pkg\nDataToolkitCore.read1","category":"page"},{"location":"internals/#DataToolkitCore.typeify","page":"Internals","title":"DataToolkitCore.typeify","text":"typeify(qt::QualifiedType; mod::Module=Main)\n\nConvert qt to a Type available in mod, if possible. If this cannot be done, nothing is returned instead.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.reinit","page":"Internals","title":"DataToolkitCore.reinit","text":"reinit(dta::AdviceAmalgamation)\n\nCheck that dta is well initialised before using it.\n\nThis does noting if dta.plugins_wanted is the same as dta.plugins_used.\n\nWhen they differ, it re-builds the advisors function list based on the currently available plugins, and updates dta.plugins_used.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore._dataadvisecall","page":"Internals","title":"DataToolkitCore._dataadvisecall","text":"_dataadvisecall(func::Function, args...; kwargs...)\n\nIdentify the first data-like argument of args (i.e. a DataCollection, DataSet, or DataTransformer), obtain its advise, and perform an advised call of func(args...; kwargs...).\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.strip_stacktrace_advice!","page":"Internals","title":"DataToolkitCore.strip_stacktrace_advice!","text":"strip_stacktrace_advice!(st::Vector{Base.StackTraces.StackFrame})\n\nRemove stack frames related to @advise and invokepkglatest from st.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.get_package","page":"Internals","title":"DataToolkitCore.get_package","text":"get_package(pkg::Base.PkgId)\nget_package(from::Module, name::Symbol)\n\nObtain a module specified by either pkg or identified by name and declared by from. Should the package not be currently loaded DataToolkit will attempt to lazy-load the package and return its module.\n\nFailure to either locate name or require pkg will result in an exception being thrown.\n\nSee also: @require, @addpkg, try_install_pkg.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.try_install_pkg","page":"Internals","title":"DataToolkitCore.try_install_pkg","text":"try_install_pkg(pkg::Base.PkgId)\n\nAttempt to install the package identified by pkg if it is not currently installed.\n\nThis function is called automatically by get_package if the package is not currently loaded, and calls Pkg's try_prompt_pkg_add method from its REPLExt package extension. If the REPL has not been loaded, nothing will be done.\n\n\n\n\n\n","category":"function"},{"location":"internals/#DataToolkitCore.read1","page":"Internals","title":"DataToolkitCore.read1","text":"read1(dataset::DataSet, as::Type)\n\nThe advisable implementation of read(dataset::DataSet, as::Type), which see.\n\nThis is essentially an exercise in useful indirection.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Consts","page":"Internals","title":"Consts","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"DataToolkitCore.LATEST_DATA_CONFIG_VERSION\nDataToolkitCore.PLUGINS_DOCUMENTATION\nDataToolkitCore.DEFAULT_PLUGINS\nDataToolkitCore.PLUGINS\nDataToolkitCore.TRANSFORMER_DOCUMENTATION\nDataToolkitCore.EXTRA_PACKAGES\nDataToolkitCore.DEFAULT_DATATRANSFORMER_PRIORITY\nDataToolkitCore.DEFAULT_DATA_ADVISOR_PRIORITY\nDataToolkitCore.DATASET_REFERENCE_WRAPPER\nDataToolkitCore.DATASET_REFERENCE_REGEX\nDataToolkitCore.QUALIFIED_TYPE_CACHE\nDataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES\nDataToolkitCore.DATA_CONFIG_KEY_SORT_MAPPING\nDataToolkitCore.LINT_SEVERITY_MESSAGES\nDataToolkitCore.LINT_SEVERITY_MAPPING\nDataToolkitCore.DEFAULT_LOG_DELAY\nDataToolkitCore.SIMPLIFY_STACKTRACES","category":"page"},{"location":"internals/#DataToolkitCore.LATEST_DATA_CONFIG_VERSION","page":"Internals","title":"DataToolkitCore.LATEST_DATA_CONFIG_VERSION","text":"The DataCollection.version set on all created DataCollections, and assumed when reading any Data.toml files which do not set data_config_version.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.PLUGINS_DOCUMENTATION","page":"Internals","title":"DataToolkitCore.PLUGINS_DOCUMENTATION","text":"A mapping from Plugin names to the documentation of said plugin.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DEFAULT_PLUGINS","page":"Internals","title":"DataToolkitCore.DEFAULT_PLUGINS","text":"The set of plugins (by name) that should used by default when creating a new data collection.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.PLUGINS","page":"Internals","title":"DataToolkitCore.PLUGINS","text":"The set of plugins currently available.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.TRANSFORMER_DOCUMENTATION","page":"Internals","title":"DataToolkitCore.TRANSFORMER_DOCUMENTATION","text":"List of (category::Symbol, named::Symbol) => docs::Any forms.\n\ncategory can be :storage, :loader, or :writer.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.EXTRA_PACKAGES","page":"Internals","title":"DataToolkitCore.EXTRA_PACKAGES","text":"The set of packages loaded by each module via @addpkg, for import with @require.\n\nMore specifically, when a module M invokes @addpkg pkg id then EXTRA_PACKAGES[M][pkg] = id is set, and then this information is used with @require to obtain the package from the root module.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DEFAULT_DATATRANSFORMER_PRIORITY","page":"Internals","title":"DataToolkitCore.DEFAULT_DATATRANSFORMER_PRIORITY","text":"The default priority field value for instances of DataTransformer.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DEFAULT_DATA_ADVISOR_PRIORITY","page":"Internals","title":"DataToolkitCore.DEFAULT_DATA_ADVISOR_PRIORITY","text":"The default priority field value for Advices.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DATASET_REFERENCE_WRAPPER","page":"Internals","title":"DataToolkitCore.DATASET_REFERENCE_WRAPPER","text":"A tuple of delimiters defining a dataset reference. For example, if set to (\"{\", \"}\") then {abc} would be recognised as a dataset reference for abc.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DATASET_REFERENCE_REGEX","page":"Internals","title":"DataToolkitCore.DATASET_REFERENCE_REGEX","text":"A regex which matches dataset references. This is constructed from DATASET_REFERENCE_WRAPPER.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.QUALIFIED_TYPE_CACHE","page":"Internals","title":"DataToolkitCore.QUALIFIED_TYPE_CACHE","text":"QUALIFIED_TYPE_CACHE\n\nA cache of QualifiedType instances, indexed by the type they represent.\n\nWhile one would hope that QualifiedType(::Type) calls would be constant-folded, in practice this is not the case, and so this cache is used to avoid an unfortunate large performance hit when constructing many QualifiedType instances.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES","page":"Internals","title":"DataToolkitCore.DATA_CONFIG_RESERVED_ATTRIBUTES","text":"The data specification TOML format constructs a DataCollection, which itself contains DataSets, comprised of metadata and DataTransformers.\n\nDataCollection\n├─ DataSet\n│  ├─ DataTransformer\n│  └─ DataTransformer\n├─ DataSet\n⋮\n\nWithin each scope, there are certain reserved attributes. They are listed in this Dict under the following keys:\n\n:collection for DataCollection\n:dataset for DataSet\n:transformer for DataTransformer\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DATA_CONFIG_KEY_SORT_MAPPING","page":"Internals","title":"DataToolkitCore.DATA_CONFIG_KEY_SORT_MAPPING","text":"When writing data configuration TOML file, the keys are (recursively) sorted. Some keys are particularly important though, and so to ensure they are placed higher a mappings from such keys to a higher sort priority string can be registered here.\n\nFor example, \"config\" => \"\\0x01\" ensures that the special configuration section is placed before all of the data sets.\n\nThis can cause odd behaviour if somebody gives a dataset the same name as a special key, but frankly that would be a bit silly (given the key names, e.g. \"uuid\") and so this is of minimal concern.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.LINT_SEVERITY_MESSAGES","page":"Internals","title":"DataToolkitCore.LINT_SEVERITY_MESSAGES","text":"A mapping from severity numbers (see LINT_SEVERITY_MAPPING) to a tuple giving the color the message should be accented with and the severity title string.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.LINT_SEVERITY_MAPPING","page":"Internals","title":"DataToolkitCore.LINT_SEVERITY_MAPPING","text":"A mapping from severity symbols to integers.\n\nThis is used to assist with more readable construction of LintItems.\n\nSee also: LINT_SEVERITY_MESSAGES for the reverse mapping of integer to severity title string.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.DEFAULT_LOG_DELAY","page":"Internals","title":"DataToolkitCore.DEFAULT_LOG_DELAY","text":"The delay in seconds before a log message is displayed.\n\nWhen zero or less, log messages are displayed immediately.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#DataToolkitCore.SIMPLIFY_STACKTRACES","page":"Internals","title":"DataToolkitCore.SIMPLIFY_STACKTRACES","text":"Whether stacktraces should be simplified by removing likely incidental DataToolkit-internals frames when displayed.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"EditURL=\"datasets.org\"","category":"page"},{"location":"datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"DataSet","category":"page"},{"location":"datasets/#DataToolkitCore.DataSet","page":"Datasets","title":"DataToolkitCore.DataSet","text":"DataSet\n\nA named collection of data, along with the means to retrive the source and interpret in into a useful form.\n\n╭╴DataSet(name, UUID) ─▶ DataCollection╶─╮\n│ ├╴Loaders: DataLoader,  […]            │\n│ │  ╰╌◁╌╮                               │\n│ ├╴Storage: DataStorage, […]            │\n│ │  ╰╌◁╌╮                               │\n│ └╴Writers: DataWriter,  […]            │\n├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Parameters(…)                          │\n╰────────────────────────────────────────╯\n\nMore concretely, a DataSet:\n\nBelongs to a DataCollection\nIs identified by its name and a UUID\nHolds any number of key-value parameters\nContains any number of DataStorage, DataLoader, and DataWriter transformers\n\nThe name or UUID can of a DataSet can be used (optionally with a given DataCollection) to create an serializable Identifier that is able to be resolved back to the DataSet in question.\n\nThe storage of a DataSet can be accessed with open(::DataSet, ::Type), and loaded with read(::DataSet, ::Type).\n\nA DataSet can be directly instantiated using the method\n\nDataSet(collection::DataCollection, name::String, uuid::UUID,\n        parameter::Dict{String, Any}, storage::Vector{DataStorage},\n        loaders::Vector{DataLoader}, writers::Vector{DataWriter})\n\nbut it is generally going to be more convenient to use create or create! depending on whether you want the created dataset to be registered in the DataCollection passed.\n\nA DataSet can be also constructed from a TOML specification using fromspec, and a TOML spec created with tospec.\n\nTransformers can be added to a DataSet with create! or the dedicated methods storage!, loader!, and writer!.\n\nSee also: DataCollection, DataStorage, DataLoader, DataWriter.\n\n\n\n\n\n","category":"type"},{"location":"datasets/#Working-with-datasets","page":"Datasets","title":"Working with datasets","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"dataset\nread\nwrite\nopen","category":"page"},{"location":"datasets/#DataToolkitCore.dataset","page":"Datasets","title":"DataToolkitCore.dataset","text":"dataset([collection::DataCollection], identstr::AbstractString, [parameters::Dict{String, Any}])\ndataset([collection::DataCollection], identstr::AbstractString, [parameters::Pair{String, Any}...])\n\nReturn the data set identified by identstr, optionally specifying the collection the data set should be found in and any parameters that apply.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Base.read","page":"Datasets","title":"Base.read","text":"read(filename::AbstractString, DataCollection; writer::Union{Function, Nothing})\n\nRead the entire contents of a file as a DataCollection.\n\nThe default value of writer is self -> write(filename, self).\n\n\n\n\n\nread(io::IO, DataCollection; path::Union{String, Nothing}=nothing, mod::Module=Base.Main)\n\nRead the entirety of io, as a DataCollection.\n\n\n\n\n\nread(dataset::DataSet, as::Type)\nread(dataset::DataSet) # as default type\n\nObtain information from dataset in the form of as, with the appropriate loader and storage provider automatically determined.\n\nThis executes the following component of the overall data flow:\n\n                 ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n\nThe loader and storage provider are selected by identifying the highest priority loader that can be satisfied by a storage provider. What this looks like in practice is illustrated in the diagram below.\n\n      read(dataset, Matrix) ⟶ ::Matrix ◀╮\n         ╭───╯        ╰────────────▷┬───╯\n╔═════╸dataset╺══════════════════╗  │\n║ STORAGE      LOADERS           ║  │\n║ (⟶ File)─┬─╮ (File ⟶ String)   ║  │\n║ (⟶ IO)   ┊ ╰─(File ⟶ Matrix)─┬─╫──╯\n║ (⟶ File)┄╯   (IO ⟶ String)   ┊ ║\n║              (IO ⟶ Matrix)╌╌╌╯ ║\n╚════════════════════════════════╝\n\n  ─ the load path used\n  ┄ an option not taken\n\nThe types that a DataSet can be loaded as are determined by the loaders, their declared types, and the implemented methods. If a method exists that can load dataset to a subtype of as, it will be used. Methods that produce a type declared in dataset's loaders are preferred.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Base.write","page":"Datasets","title":"Base.write","text":"write(dataset::DataSet, info::Any)\n\nTODO write docstring\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Base.open","page":"Datasets","title":"Base.open","text":"open(dataset::DataSet, as::Type; write::Bool=false)\n\nObtain the data of dataset in the form of as, with the appropriate storage provider automatically selected.\n\nA write flag is also provided, to help the driver pick a more appropriate form of as.\n\nThis executes the following component of the overall data flow:\n\n                 ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n\n\n\n\n\n","category":"function"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"create\ncreate!\nloader!\nstorage!\nwriter!","category":"page"},{"location":"datasets/#DataToolkitCore.create","page":"Datasets","title":"DataToolkitCore.create","text":"create(parent::DataCollection, ::Type{DataSet}, name::AbstractString, specification::Dict{String, <:Any})\ncreate(parent::DataCollection, ::Type{DataSet}, name::AbstractString, specification::Pair{String, <:Any}...)\n\nCreate a new DataSet that is a child of parent with a given name and specification.\n\nSee also: create!.\n\n\n\n\n\ncreate(parent::DataSet, T::Type{<:DataTransformer}, spec::Dict{String, <:Any})\ncreate(parent::DataSet, T::Type{<:DataTransformer}, driver::Symbol, spec::Dict{String, <:Any})\ncreate(parent::DataSet, T::Type{<:DataTransformer}, driver::Symbol, specs::Pair{String, <:Any}...)\n\nCreate a new data transformer of type T that is a child of the parent dataset, with a given specification spec.\n\nThe driver argument may be explicitly specified as a symbol, or it may be included as part of spec.\n\nSee also: create!.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.create!","page":"Datasets","title":"DataToolkitCore.create!","text":"create!(::Type{DataCollection}, name::Union{String, Nothing}, path::Union{String, Nothing};\n        uuid::UUID=uuid4(), plugins::Vector{String}=String[], mod::Module=Base.Main)\n\nCreate a new data collection.\n\nThis can be an in-memory data collection, when path is set to nothing, or a collection which corresponds to a Data TOML file, in which case path should be set to either a path to a .toml file or an existing directory in which a Data.toml file should be placed.\n\nWhen a path is provided, the data collection will immediately be written, overwriting any existing file at the path.\n\n\n\n\n\ncreate!(parent::DataSet, ::Type{DataSet}, name::AbstractString, specification::Dict{String, <:Any})\ncreate!(parent::DataSet, ::Type{DataSet}, name::AbstractString, specification::Pair{String, <:Any}...)\n\nCreate a new DataSet that is a child of parent with a given name and specification, and add it to the parent's list of datasets.\n\nSee also: create.\n\n\n\n\n\ncreate!(parent::DataSet, T::Type{<:DataTransformer}, spec::Dict{String, <:Any})\ncreate!(parent::DataSet, T::Type{<:DataTransformer}, driver::Symbol, spec::Dict{String, <:Any})\ncreate!(parent::DataSet, T::Type{<:DataTransformer}, driver::Symbol, specs::Pair{String, <:Any}...)\n\nCreate a new data transformer of type T that is a child of the parent dataset, with a given specification spec, and add it to the appropriate list of transformers.\n\nSee also: create, loader!, storage!, writer!.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.loader!","page":"Datasets","title":"DataToolkitCore.loader!","text":"loader!(dataset::DataSet, driver::Symbol, parameters::Dict{String, <:Any})\nloader!(dataset::DataSet, driver::Symbol, parameters::Pair{String, <:Any}...)\n\nCreate a new data loader transformer that is a child of the dataset dataset, with a given driver driver and specification parameters, and add it to the dataset's list of loader transformers.\n\nSee also: create!, storage!, writer!.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.storage!","page":"Datasets","title":"DataToolkitCore.storage!","text":"storage!(dataset::DataSet, driver::Symbol, parameters::Dict{String, <:Any})\nstorage!(dataset::DataSet, driver::Symbol, parameters::Pair{String, <:Any}...)\n\nCreate a new data storage transformer that is a child of the dataset dataset, with a given driver driver and specification parameters, and add it to the dataset's list of storage transformers.\n\nSee also: create!, loader!, writer!.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.writer!","page":"Datasets","title":"DataToolkitCore.writer!","text":"writer!(dataset::DataSet, driver::Symbol, parameters::Dict{String, <:Any})\nwriter!(dataset::DataSet, driver::Symbol, parameters::Pair{String, <:Any}...)\n\nCreate a new data writer transformer that is a child of the dataset dataset, with a given driver driver and specification parameters, and add it to the dataset's list of writer transformers.\n\nSee also: create!, storage!, loader!.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Dataset-identification","page":"Datasets","title":"Dataset identification","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"Identifier\nresolve\nrefine","category":"page"},{"location":"datasets/#DataToolkitCore.Identifier","page":"Datasets","title":"DataToolkitCore.Identifier","text":"Identifier\n\nA description that can be used to uniquely identify a DataSet.\n\nFour fields are used to describe the target DataSet:\n\ncollection, the name or UUID of the collection (optional).\ndataset, the name or UUID of the dataset.\ntype, the type that should be loaded from the dataset.\nparameters, any extra parameters of the dataset that should match.\n\nSee also: resolve, refine.\n\nConstructors\n\nIdentifier(collection::Union{AbstractString, UUID, Nothing},\n           dataset::Union{AbstractString, UUID},\n           type::Union{QualifiedType, Nothing},\n           parameters::Dict{String, Any})\n\nParsing\n\nAn Identifier can be represented as a string with the following form, with the optional components enclosed by square brackets:\n\n[COLLECTION:]DATASET[::TYPE]\n\nSuch forms can be parsed to an Identifier by simply calling the parse function, i.e. parse(Identifier, \"mycollection:dataset\").\n\n\n\n\n\n","category":"type"},{"location":"datasets/#DataToolkitCore.resolve","page":"Datasets","title":"DataToolkitCore.resolve","text":"resolve(collection::DataCollection, ident::Identifier;\n        resolvetype::Bool=true, requirematch::Bool=true)\n\nAttempt to resolve an identifier (ident) to a particular data set. Matching data sets will searched for from collection.\n\nWhen resolvetype is set and ident specifies a datatype, the identified data set will be read to that type.\n\nWhen requirematch is set an error is raised should no dataset match ident. Otherwise, nothing is returned.\n\n\n\n\n\nresolve(ident::Identifier; resolvetype::Bool=true, stack=STACK)\n\nAttempt to resolve ident using the specified data layer, if present, trying every layer of the data stack in turn otherwise.\n\n\n\n\n\nresolve(identstr::AbstractString, parameters::Union{Dict{String, Any}, Nothing}=nothing;\n        resolvetype::Bool=true, stack::Vector{DataCollection}=STACK)\n\nAttempt to resolve the identifier given by identstr and parameters against each layer of the data stack in turn.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.refine","page":"Datasets","title":"DataToolkitCore.refine","text":"refine(collection::DataCollection, datasets::Vector{DataSet}, ident::Identifier)\n\nFilter datasets (from collection) to data sets than match the identifier ident.\n\nThis function contains an advise entrypoint where plugins can apply further filtering, applied to the method refine(::Vector{DataSet}, ::Identifier, ::Vector{String}).\n\n\n\n\n\nrefine(datasets::Vector{DataSet}, ::Identifier, ignoreparams::Vector{String})\n\nThis is a stub function that exists soley as as an advise point for data set filtering during resolution of an identifier.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Data-collections","page":"Datasets","title":"Data collections","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"DataCollection\nloadcollection!\nSTACK\niswritable(::DataCollection)","category":"page"},{"location":"datasets/#DataToolkitCore.DataCollection","page":"Datasets","title":"DataToolkitCore.DataCollection","text":"DataCollection\n\nA collection of DataSets, with global configuration, Plugins, and a few other extras.\n\n╭╴DataCollection(name, UUID, path, module)╶─╮\n│ ├╴DataSet(…)                              │\n│ ├╴DataSet                                 │\n│ │ ├╴Loaders: DataLoader,  […]             │\n│ │ │  ╰╌◁╌╮                                │\n│ │ ├╴Storage: DataStorage, […]             │\n│ │ │  ╰╌◁╌╮                                │\n│ │ └╴Writers: DataWriter,  […]             │\n│ ⋮                                         │\n├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Plugins(…)                                │\n├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ Parameters(…)                             │\n╰───────────────────────────────────────────╯\n\nWorking with DataCollections\n\nIt is usual for non-transient DataCollections to be put onto the \"STACK\" (this is done automatically by loadcollection!). This is a collection of globally known DataCollections.\n\nBeing on the STACK allows a dataset to be retrieved by its name or UUID using getlayer, and for a DataSets in one DataCollection to refer to a DataSet in another.\n\nWhen using the data> REPL mode, the top collection on the stack is used as a default target for all operations.\n\nCreating a DataCollection\n\nA Data.toml file can be loaded as a DataCollection (and put on the STACK) with loadcollection!.\n\nTo programatically create a DataToolkit you can either call the full constructor, but that's rather involved, and so a more convenient constructor is also defined:\n\nDataCollection(name::Union{String, Nothing}, [parameters::Dict{String, Any}];\n               path::Union{String, Nothing} = nothing,\n               uuid::UUID = uuid4(),\n               plugins::Vector{String} = String[],\n               mod::Module = Base.Main,\n               parameters...) -> DataCollection\n\nNote that parameters can either be provided as the second positional argument, or extra keyword arguments, but not both.\n\nOnce a DataCollection has been created, DataSets can be added to it with create!.\n\nExamples\n\njulia> DataCollection(\"test\")\nDataCollection: test\n  Data sets:\n\njulia> c1 = DataCollection(nothing, Dict(\"customparam\" => 77))\nDataCollection:\n  Data sets:\n\njulia> c2 = DataCollection(\"test2\", plugins = [\"defaults\", \"store\"], customparam=77)\nDataCollection: test2\n  Plugins: defaults ✔, store ✔\n  Data sets:\n\njulia> c1.parameters\nDict{String, Any} with 1 entry:\n  \"customparam\" => 77\n\njulia> c2.parameters\nDict{String, Any} with 1 entry:\n  \"customparam\" => 77\n\nSaving a DataCollection\n\nAfter modifying a file-backed DataCollection, the file can be updated by calling write(::DataCollection) (so long as iswritable(::DataCollection) is true).\n\nAny DataCollection can also be written to a particular destination with write(dest, ::DataCollection).\n\nWriting a DataCollection to plaintext is essentially performed by calling TOML.print on the result of convert(::Type{Dict}, ::DataCollection).\n\nFields\n\nversion::Int\nname::Union{String, Nothing}\nuuid::UUID\nplugins::Vector{String}\nparameters::Dict{String, Any}\ndatasets::Vector{DataSet}\npath::Union{String, Nothing}\nadvise::AdviceAmalgamation\nmod::Module\n\n\n\n\n\n","category":"type"},{"location":"datasets/#DataToolkitCore.loadcollection!","page":"Datasets","title":"DataToolkitCore.loadcollection!","text":"loadcollection!(source::Union{<:AbstractString, <:IO}, mod::Module=Base.Main;\n                soft::Bool=false, index::Int=1)\n\nLoad a data collection from source and add it to the data stack at index. source must be accepted by read(source, DataCollection).\n\nmod should be set to the Module within which loadcollection! is being invoked. This is important when code is run by the collection. As such, it is usually appropriate to call:\n\nloadcollection!(source, @__MODULE__; soft)\n\nWhen soft is set, should an data collection already exist with the same UUID, nothing will be done and nothing will be returned.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.STACK","page":"Datasets","title":"DataToolkitCore.STACK","text":"The set of data collections currently available.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Base.iswritable-Tuple{DataCollection}","page":"Datasets","title":"Base.iswritable","text":"iswritable(dc::DataCollection)\n\nCheck whether the data collection dc is backed by a writable file.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Manipulation-API","page":"Datasets","title":"Manipulation API","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"getlayer\nstack_index\nstack_move\nstack_remove!\nplugin_add\nplugin_remove\nplugin_info\nplugin_list\nconfig_get\nconfig_set\nconfig_unset\ndelete!\nreplace!","category":"page"},{"location":"datasets/#DataToolkitCore.getlayer","page":"Datasets","title":"DataToolkitCore.getlayer","text":"getlayer([::Nothing])\n\nReturn the first DataCollection on the STACK.\n\n\n\n\n\ngetlayer(name::AbstractString)\ngetlayer(uuid::UUID)\n\nFind the DataCollection in STACK with name/uuid.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.stack_index","page":"Datasets","title":"DataToolkitCore.stack_index","text":"stack_index(ident::Union{Int, String, UUID, DataCollection}; quiet::Bool=false)\n\nObtain the index of the data collection identified by ident on the stack, if it is present. If it is not found, nothing is returned and unless quiet is set a warning is printed.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.stack_move","page":"Datasets","title":"DataToolkitCore.stack_move","text":"stack_move(ident::Union{Int, String, UUID, DataCollection}, shift::Int; quiet::Bool=false)\n\nFind ident in the data collection stack, and shift its position by shift, returning the new index. shift is clamped so that the new index lies within STACK.\n\nIf ident could not be resolved, then nothing is returned and unless quiet is set a warning is printed.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.stack_remove!","page":"Datasets","title":"DataToolkitCore.stack_remove!","text":"stack_remove!(ident::Union{Int, String, UUID, DataCollection}; quiet::Bool=false)\n\nFind ident in the data collection stack and remove it from the stack, returning the index at which it was found.\n\nIf ident could not be resolved, then nothing is returned and unless quiet is set a warning is printed.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.plugin_add","page":"Datasets","title":"DataToolkitCore.plugin_add","text":"plugin_add([collection::DataCollection=first(STACK)], plugins::Vector{<:AbstractString};\n           quiet::Bool=false)\n\nReturn a variation of collection with all plugins not currently used added to the plugin list.\n\nUnless quiet is a set an informative message is printed.\n\nwarning: Side effects\nThe new collection is written, if possible.Should collection be part of STACK, the stack entry is updated in-place.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.plugin_remove","page":"Datasets","title":"DataToolkitCore.plugin_remove","text":"plugin_remove([collection::DataCollection=first(STACK)], plugins::Vector{<:AbstractString};\n              quiet::Bool=false)\n\nReturn a variation of collection with all plugins currently used removed from the plugin list.\n\nUnless quiet is a set an informative message is printed.\n\nwarning: Side effects\nThe new collection is written, if possible.Should collection be part of STACK, the stack entry is updated in-place.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.plugin_info","page":"Datasets","title":"DataToolkitCore.plugin_info","text":"plugin_info(plugin::AbstractString; quiet::Bool=false)\n\nFetch the documentation of plugin, or return nothing if documentation could not be fetched.\n\nIf quiet is not set warning messages will be omitted when no documentation could be fetched.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.plugin_list","page":"Datasets","title":"DataToolkitCore.plugin_list","text":"plugin_list(collection::DataCollection=first(STACK); quiet::Bool=false)\n\nObtain a list of plugins used in collection.\n\nquiet is unused but accepted as an argument for the sake of consistency.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.config_get","page":"Datasets","title":"DataToolkitCore.config_get","text":"config_get(propertypath::Vector{String};\n           collection::DataCollection=first(STACK), quiet::Bool=false)\n\nObtain the configuration value at propertypath in collection.\n\nWhen no value is set, nothing is returned instead and if quiet is unset \"unset\" is printed.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.config_set","page":"Datasets","title":"DataToolkitCore.config_set","text":"config_set([collection::DataCollection=first(STACK)], propertypath::Vector{String}, value::Any;\n           quiet::Bool=false)\n\nReturn a variation of collection with the configuration at propertypath set to value.\n\nUnless quiet is set, a success message is printed.\n\nwarning: Side effects\nThe new collection is written, if possible.Should collection be part of STACK, the stack entry is updated in-place.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.config_unset","page":"Datasets","title":"DataToolkitCore.config_unset","text":"config_unset([collection::DataCollection=first(STACK)], propertypath::Vector{String};\n              quiet::Bool=false)\n\nReturn a variation of collection with the configuration at propertypath removed.\n\nUnless quiet is set, a success message is printed.\n\nwarning: Side effects\nThe new collection is written, if possible.Should collection be part of STACK, the stack entry is updated in-place.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Base.delete!","page":"Datasets","title":"Base.delete!","text":"delete!(dataset::DataSet)\n\nRemove dataset from its parent collection.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Base.replace!","page":"Datasets","title":"Base.replace!","text":"replace!(dataset::DataSet; [name, uuid, parameters, storage, loaders, writers])\n\nPerform an in-place update of dataset, optionally replacing any of the name, uuid, parameters, storage, loaders, or writers fields.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#TOML-serialization","page":"Datasets","title":"TOML serialization","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"fromspec\ntospec","category":"page"},{"location":"datasets/#DataToolkitCore.fromspec","page":"Datasets","title":"DataToolkitCore.fromspec","text":"fromspec(DT::Type{<:DataTransformer}, dataset::DataSet, spec::Dict{String, Any})\n\nCreate an DT of dataset according to spec.\n\nDT can either contain the driver name as a type parameter, or it will be read from the \"driver\" key in spec.\n\n\n\n\n\nfromspec(::Type{DataCollection}, spec::Dict{String, Any};\n         path::Union{String, Nothing}=nothing, mod::Module=Base.Main)\n\nCreate a DataCollection from spec.\n\nThe path and mod keywords are used as the values for the corresponding fields in the DataCollection.\n\n\n\n\n\nfromspec(::Type{DataSet}, collection::DataCollection, name::String, spec::Dict{String, Any})\n\nCreate a DataSet for collection called name, according to spec.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#DataToolkitCore.tospec","page":"Datasets","title":"DataToolkitCore.tospec","text":"tospec(thing::DataTransformer)\ntospec(thing::DataSet)\ntospec(thing::DataCollection)\n\nReturn a Dict representation of thing for writing as TOML.\n\n\n\n\n\n","category":"function"},{"location":"datasets/#Qualified-types","page":"Datasets","title":"Qualified types","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"QualifiedType","category":"page"},{"location":"datasets/#DataToolkitCore.QualifiedType","page":"Datasets","title":"DataToolkitCore.QualifiedType","text":"QualifiedType\n\nA representation of a Julia type that does not need the type to be defined in the Julia session, and can be stored as a string. This is done by storing the type name and the module it belongs to as Symbols.\n\nwarning: Warning\nWhile QualifiedType is currently quite capable, it is not currently able to express the full gamut of Julia types. In future this will be improved, but it will likely always be restricted to a certain subset.\n\nSee also: typeify.\n\nSubtyping\n\nWhile the subtype operator cannot work on QualifiedTypes (<: is a built-in), when the Julia types are defined the subset operator ⊆ can be used instead. This works by simply converting the QualifiedTypes to the corresponding Type and then applying the subtype operator.\n\njulia> QualifiedTypes(:Base, :Vector) ⊆ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> Matrix ⊆ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> QualifiedTypes(:Base, :Vector) ⊆ AbstractVector\ntrue\n\njulia> QualifiedTypes(:Base, :Foobar) ⊆ AbstractVector\nfalse\n\nConstructors\n\nQualifiedType(parentmodule::Symbol, typename::Symbol)\nQualifiedType(t::Type)\n\nParsing\n\nA QualifiedType can be expressed as a string as \"$parentmodule.$typename\". This can be easily parsed as a QualifiedType, e.g. parse(QualifiedType, \"Core.IO\").\n\n\n\n\n\n","category":"type"}]
}
